{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Testing_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9ZhrUeTDsx-"
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import models\n",
        "from numpy import array_equal\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, SimpleRNN, GRU\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import csv\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx0oFPzV5vpB",
        "outputId": "4c56869a-23b9-4326-8daa-bb88487da87f"
      },
      "source": [
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lexicon-dataset'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 30 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "e31e4674-35a1-4f68-f64c-acc38eb7eeff"
      },
      "source": [
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_source))\n",
        "print(\"Number of testing samples: \", len(test_source))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  68218\n",
            "Number of testing samples:  6864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgxYhZkv53Rl",
        "outputId": "53f4a1d2-4d39-4622-8d4f-68e5edd522c6"
      },
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(test_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "input_texts_ns = []\n",
        "target_texts_ns = []\n",
        "test_input_texts_ns = []\n",
        "test_target_texts_ns = []\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_texts_ns.append(input_text)\n",
        "    target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(test_source, test_target):\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    test_input_texts_ns.append(input_text)\n",
        "    test_target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_texts.append(input_texts_ns[arr[i]])\n",
        "    target_texts.append(target_texts_ns[arr[i]])\n",
        "\n",
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "for i in range(len(test_source)):\n",
        "    test_input_texts.append(test_input_texts_ns[arr1[i]])\n",
        "    test_target_texts.append(test_target_texts_ns[arr1[i]])\n",
        "\n",
        "input_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Adding the padding character\n",
        "#input_characters.append(\"P\")\n",
        "#target_characters.append(\"P\")\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "test_max_encoder_seq_length = max([len(txt) for txt in test_input_texts])\n",
        "test_max_decoder_seq_length = max([len(txt) for txt in test_target_texts])\n",
        "\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Max sequence length for val inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Max sequence length for val outputs:\", test_max_decoder_seq_length)\n",
        "\n",
        "print(input_characters)\n",
        "print(target_characters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 68218\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Max sequence length for val inputs: 23\n",
            "Max sequence length for val outputs: 24\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'B', 'E', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtHaPQPw6VuP",
        "outputId": "9331ae27-9031-4b25-8725-0e7a31661bd5"
      },
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "reverse_source_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "print(input_token_index)\n",
        "print(target_token_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'B': 1, 'E': 2, 'ஃ': 3, 'அ': 4, 'ஆ': 5, 'இ': 6, 'ஈ': 7, 'உ': 8, 'ஊ': 9, 'எ': 10, 'ஏ': 11, 'ஐ': 12, 'ஒ': 13, 'ஓ': 14, 'க': 15, 'ங': 16, 'ச': 17, 'ஜ': 18, 'ஞ': 19, 'ட': 20, 'ண': 21, 'த': 22, 'ந': 23, 'ன': 24, 'ப': 25, 'ம': 26, 'ய': 27, 'ர': 28, 'ற': 29, 'ல': 30, 'ள': 31, 'ழ': 32, 'வ': 33, 'ஷ': 34, 'ஸ': 35, 'ஹ': 36, 'ா': 37, 'ி': 38, 'ீ': 39, 'ு': 40, 'ூ': 41, 'ெ': 42, 'ே': 43, 'ை': 44, 'ொ': 45, 'ோ': 46, 'ௌ': 47, '்': 48}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtAojV-O6p0G"
      },
      "source": [
        "trunc_input_texts = input_texts[:68096]\n",
        "trunc_target_texts = target_texts[:68096]\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(trunc_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(trunc_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(trunc_input_texts, trunc_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_target_data[i, t, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    \n",
        "test_encoder_input_data = np.zeros(\n",
        "    (len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "test_decoder_target_data = np.zeros(\n",
        "    (len(test_target_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    test_encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "      # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        test_decoder_target_data[i, t, target_token_index[char]] = 1.0\n",
        "    test_decoder_target_data[i, t + 1: ,target_token_index[\" \"]] = 1.0\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx8rEIqmMQNT"
      },
      "source": [
        "# ATTENTION MECHANISM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-RyyRhTQ2XC"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self, query, values):\n",
        "    \n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    \n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYLhbKsxt3EH"
      },
      "source": [
        "#import tensorflow \n",
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "   \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
        "    \n",
        "    #LUONGH Dot-product\n",
        "    score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mfk2-afoCeE"
      },
      "source": [
        "class MyRNN_atten(object):\n",
        "  def __init__(self,cell_type = 'RNN', hidden_size=32, \n",
        "               learning_rate= 1e-3,dropout=0.3,epochs = 10, batch_size = 32,\n",
        "               attention = 'bahdanau'):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.attention = attention\n",
        "\n",
        "  def build_fit(self,encoder_input_data,decoder_target_data):\n",
        "\n",
        "    encoder_inputs = Input(shape=(max_encoder_seq_length, num_encoder_tokens), name='encoder_inputs')\n",
        "    if self.cell_type == 'LSTM':\n",
        "      encoder_lstm = LSTM(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_lstm')\n",
        "      encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h, encoder_state_c]\n",
        "    elif self.cell_type == 'GRU':\n",
        "      encoder_gru = GRU(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_gru')\n",
        "      encoder_outputs, encoder_state_h = encoder_gru(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h]\n",
        "    elif self.cell_type == 'RNN':\n",
        "      encoder_rnn = SimpleRNN(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_rnn')\n",
        "      encoder_outputs, encoder_state_h = encoder_rnn(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h]\n",
        "\n",
        "    # Set up the attention layer\n",
        "    if self.attention == 'bahdanau':\n",
        "      attention= BahdanauAttention(self.hidden_size)\n",
        "    elif self.attention == 'luong':\n",
        "      attention= LuongAttention(self.hidden_size)\n",
        "\n",
        "    # Set up the decoder layers\n",
        "    decoder_inputs = Input(shape=(1, (num_decoder_tokens+self.hidden_size)),name='decoder_inputs')\n",
        "    if self.cell_type == 'LSTM':\n",
        "      decoder_lstm = LSTM(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_lstm')\n",
        "    elif self.cell_type == 'GRU':\n",
        "      decoder_gru = GRU(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_gru')\n",
        "    elif self.cell_type == 'RNN':\n",
        "      decoder_rnn = SimpleRNN(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_rnn')  \n",
        "    \n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax',  name='decoder_dense')\n",
        "\n",
        "    all_outputs = []\n",
        "\n",
        "    inputs = np.zeros((self.batch_size, 1, num_decoder_tokens))\n",
        "    inputs[:, 0, 0] = 1 \n",
        "\n",
        "    decoder_outputs = encoder_state_h\n",
        "    states = encoder_states\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "\n",
        "      context_vector, attention_weights=attention(decoder_outputs, encoder_outputs)\n",
        "      \n",
        "      context_vector = tf.expand_dims(context_vector, 1)\n",
        "      \n",
        "      inputs = tf.concat([context_vector, inputs], axis=-1)\n",
        "      if self.cell_type == 'LSTM':\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)\n",
        "      if self.cell_type == 'GRU':\n",
        "        decoder_outputs, state_h = decoder_gru(inputs, initial_state=states)\n",
        "      if self.cell_type == 'RNN':\n",
        "        decoder_outputs, state_h = decoder_rnn(inputs, initial_state=states)\n",
        "      \n",
        "      outputs = decoder_dense(decoder_outputs)\n",
        "      outputs = tf.expand_dims(outputs, 1)\n",
        "      all_outputs.append(outputs)\n",
        "      inputs = outputs\n",
        "      if self.cell_type == 'LSTM':\n",
        "        states = [state_h, state_c]\n",
        "      if self.cell_type == 'GRU' or self.cell_type == 'RNN':\n",
        "        states = [state_h]\n",
        "\n",
        "\n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    #getindicelayer = Lambda(lambda x: x[:, -1, :]) \n",
        "    #decoder_outputs = getindicelayer(all_outputs)\n",
        "\n",
        "    model = Model(encoder_inputs, decoder_outputs, name='model_encoder_decoder')\n",
        "    \n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    #model.summary()\n",
        "\n",
        "    model.fit(encoder_input_data, decoder_target_data,\n",
        "              batch_size=self.batch_size, \n",
        "              epochs=self.epochs,\n",
        "              #callbacks = [WandbCallback()]\n",
        "              )\n",
        "    \n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    test_count = 6784\n",
        "\n",
        "    pred = model.predict(test_encoder_input_data[:test_count], batch_size = self.batch_size)\n",
        "\n",
        "    data_list = [[\"SNO\", \"Input Data\", \"Target Data\", \"Predicted Data\"]]\n",
        "\n",
        "    for index in range(0,test_count):\n",
        "      pred_vector = pred[index]\n",
        "      true_vector = test_decoder_target_data[index]\n",
        "      pred_indices = tf.argmax(pred_vector, axis=1)\n",
        "      true_indices = tf.argmax(true_vector, axis=1)\n",
        "\n",
        "      if (pred_indices.numpy() == true_indices.numpy()).all():\n",
        "        global_correct = global_correct + 1\n",
        "      \n",
        "      global_total = global_total + 1\n",
        "\n",
        "      arr = pred_indices.numpy()\n",
        "      decoded_sequence = ''\n",
        "      for i in range(1,len(arr)):\n",
        "        if arr[i] != 2:\n",
        "          decoded_sequence = decoded_sequence + reverse_target_char_index[arr[i]]\n",
        "\n",
        "      true_word = test_target_texts[index] \n",
        "      true_word = true_word[1:len(true_word)-1]\n",
        "      #print(true_word)\n",
        "      #print(decoded_sequence)\n",
        "      dlist = [index+1, test_input_texts[index], true_word, decoded_sequence]\n",
        "      data_list.append(dlist)\n",
        "\n",
        "    with open('predictions_attention.tsv', 'w', newline='') as file:\n",
        "      writer = csv.writer(file, delimiter='\\t')\n",
        "      writer.writerows(data_list)\n",
        "\n",
        "    val_accuracy = global_correct/global_total\n",
        "    print(val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYNf5iRhCiAv"
      },
      "source": [
        "# BEST HYPERPARAMETERS\n",
        "\n",
        "best_attention = 'bahdanau'\n",
        "best_batch_size = 128\n",
        "best_cell_type = 'RNN'\n",
        "best_dropout = 0\n",
        "best_epochs = 20\n",
        "best_hidden_size = 128\n",
        "best_learning_rate = 0.001\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H8C__mqymd-"
      },
      "source": [
        "model_rnn = MyRNN_atten(cell_type = best_cell_type, hidden_size=best_hidden_size, learning_rate= best_learning_rate,\n",
        "                        dropout=best_dropout,epochs = best_epochs, batch_size = best_batch_size, attention = best_attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2iRrBYTEoCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f20a79e-7c1e-4b86-c262-636bc8ff7258"
      },
      "source": [
        "model_rnn.build_fit(encoder_input_data,decoder_target_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "532/532 [==============================] - 52s 60ms/step - loss: 1.3523 - accuracy: 0.6689\n",
            "Epoch 2/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.8321 - accuracy: 0.7436\n",
            "Epoch 3/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.5252 - accuracy: 0.8403\n",
            "Epoch 4/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.3607 - accuracy: 0.9077\n",
            "Epoch 5/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.3069 - accuracy: 0.9235\n",
            "Epoch 6/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.2830 - accuracy: 0.9291\n",
            "Epoch 7/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2657 - accuracy: 0.9334\n",
            "Epoch 8/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2646 - accuracy: 0.9331\n",
            "Epoch 9/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2438 - accuracy: 0.9388\n",
            "Epoch 10/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2527 - accuracy: 0.9366\n",
            "Epoch 11/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2598 - accuracy: 0.9354\n",
            "Epoch 12/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2276 - accuracy: 0.9427\n",
            "Epoch 13/20\n",
            "532/532 [==============================] - 33s 61ms/step - loss: 0.2427 - accuracy: 0.9389\n",
            "Epoch 14/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2179 - accuracy: 0.9454\n",
            "Epoch 15/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.1993 - accuracy: 0.9505\n",
            "Epoch 16/20\n",
            "532/532 [==============================] - 32s 61ms/step - loss: 0.2248 - accuracy: 0.9435\n",
            "Epoch 17/20\n",
            "532/532 [==============================] - 33s 61ms/step - loss: 0.2129 - accuracy: 0.9468\n",
            "Epoch 18/20\n",
            "532/532 [==============================] - 33s 61ms/step - loss: 0.2102 - accuracy: 0.9473\n",
            "Epoch 19/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.1948 - accuracy: 0.9518\n",
            "Epoch 20/20\n",
            "532/532 [==============================] - 32s 60ms/step - loss: 0.1863 - accuracy: 0.9540\n",
            "0.49867334905660377\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
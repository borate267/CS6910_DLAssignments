{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Testing_transliteration_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNDodnmvkEb"
      },
      "source": [
        "The following code uses the best set of hyperparameters obtained after a set 87 sweeps, on the test data. The program outputs a predictions_vanilla.csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "#Importing essentials and Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcsn9UVpEyH",
        "outputId": "31f84811-790d-47ae-db75-8f25193a03b4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 13 04:28:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    33W / 250W |    841MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "import io\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from math import log1p \n",
        "import keras"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "**Fetching the dataset** \n",
        "\n",
        "Lexicons for Latin-Tamil are taken from Google's Dakshina dataset. The necessary datasets have been uploaded to github, cloned and used for the reminder of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "8cc7708a-7917-47ce-daf9-54ad98f2b7e8"
      },
      "source": [
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lexicon-dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "print_data = True #Set to False if you do not want to print "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "#Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzvL2e20Sx-"
      },
      "source": [
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCH31Lo6waWp"
      },
      "source": [
        "#Shuffling the datasets, creating a vocabulary and tokenising the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mC2ICFKRUZA",
        "outputId": "5ae0eff5-2d83-49ab-a6c7-e1c22804c5b9"
      },
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(test_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "# Holds the vocabulary\n",
        "source_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# Holds unshuffled datasets\n",
        "input_texts_ns = []\n",
        "target_texts_ns = []\n",
        "test_input_texts_ns = []\n",
        "test_target_texts_ns = []\n",
        "\n",
        "# The target words are appended with B and E which stand for\n",
        "# Beginning (start sequence character) and End (end sequence character) respectively\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_texts_ns.append(input_text)\n",
        "    target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in source_characters:\n",
        "            source_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(test_source, test_target):\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    test_input_texts_ns.append(input_text)\n",
        "    test_target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in source_characters:\n",
        "            source_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "# Shuffling the datasets\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_texts.append(input_texts_ns[arr[i]])\n",
        "    target_texts.append(target_texts_ns[arr[i]])\n",
        "\n",
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "for i in range(len(test_source)):\n",
        "    test_input_texts.append(test_input_texts_ns[arr1[i]])\n",
        "    test_target_texts.append(test_target_texts_ns[arr1[i]])\n",
        "\n",
        "# Adding the padding character\n",
        "source_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "\n",
        "# Creating the vocabulary\n",
        "source_characters = sorted(list(source_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Essential parameters which will be periodically used in the reminder of the code\n",
        "\n",
        "num_encoder_tokens = len(source_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "test_max_encoder_seq_length = max([len(txt) for txt in test_input_texts])\n",
        "test_max_decoder_seq_length = max([len(txt) for txt in test_target_texts])\n",
        "\n",
        "# Tokenising elements in the vocabulary\n",
        "source_token_index = dict([(char, i) for i, char in enumerate(source_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "reverse_source_char_index = dict((i, char) for char, i in source_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Print out some data\n",
        "if (print_data):\n",
        "  print(\"Number of training samples:\", len(input_texts))\n",
        "  print(\"Number of testing samples: \", len(test_source))\n",
        "  print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "  print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "  print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "  print(\"Max sequence length for test inputs:\", test_max_encoder_seq_length)\n",
        "  print(\"Max sequence length for test outputs:\", test_max_decoder_seq_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 68218\n",
            "Number of testing samples:  6864\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Max sequence length for test inputs: 23\n",
            "Max sequence length for test outputs: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3W77CTE4tp"
      },
      "source": [
        "Character Embedding\n",
        "\n",
        "**Encoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_encSeqLen)**\n",
        "\n",
        "**Decoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen)**\n",
        "\n",
        "**Decoder Target Sequences**: Padded to a maximum length of max_decSeqLen characters with a vocabulary of sizeofTamilVocab different characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen, sizeofTamilVocab)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3i7ykYbtQ2K"
      },
      "source": [
        "# For training\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = source_token_index[char]\n",
        "    encoder_input_data[i, t + 1 :] = source_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# For testing\n",
        "test_encoder_input_data = np.zeros((len(input_texts), test_max_encoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_input_data = np.zeros((len(input_texts), test_max_decoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_target_data = np.zeros((len(input_texts), test_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_encoder_input_data[i, t] = source_token_index[char]\n",
        "    test_encoder_input_data[i, t + 1 :] = source_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        test_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    test_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    test_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui9pFxRvSdeT"
      },
      "source": [
        "#TESTING GROUND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7q6xNssrRwb"
      },
      "source": [
        "x_test = test_encoder_input_data\n",
        "y_test = test_target_texts\n",
        "#print(np.shape(y_test))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTFnlQ8NyU1"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvI1fI7UOPMT"
      },
      "source": [
        "class MyRNN(object):\n",
        "  def __init__(self,cell_type = 'RNN',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "               dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "               num_enc = 1,num_dec = 1):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.in_emb = in_emb\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.pred_type = pred_type\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.beam_width = beam_width\n",
        "    self.num_enc = num_enc\n",
        "    self.num_dec = num_dec\n",
        "\n",
        "  def build_fit(self,encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test):\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = Input(shape=(None, ),name = 'Enc_inputs')\n",
        "\n",
        "    # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and\n",
        "    # output embedding dimension of size in_enc.\n",
        "    enc_emb =  Embedding(num_encoder_tokens, self.in_emb , mask_zero = True,name = 'Enc_emb')(encoder_inputs)\n",
        "    encoder_outputs = enc_emb\n",
        "\n",
        "    # Adding num_enc number of cells of type LSTM/GRU/RNN\n",
        "    if self.cell_type == 'LSTM':\n",
        "      encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "      encoder_states = [state_h, state_c]\n",
        "\n",
        "      for i in range( 2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs,initial_state = encoder_states)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h = encoder_gru(encoder_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h = encoder_gru(encoder_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h = encoder_rnn(encoder_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None,), name = 'Dec_inputs')\n",
        "    dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero = True, name = 'Dec_emb')\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    decoder_outputs = dec_emb\n",
        "\n",
        "    # Adding num_dec number of cells of type LSTM/GRU/RNN\n",
        "    if self.cell_type == 'LSTM':\n",
        "      decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state = encoder_states)\n",
        "      \n",
        "      for i in range(2, self.num_dec +1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _ = decoder_gru(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _ = decoder_gru(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _ = decoder_rnn(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name = 'dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that takes encoder and decoder input \n",
        "    # to output decoder_outputs\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.epochs\n",
        "        )\n",
        "\n",
        "    #model.save(\"s2s\")\n",
        "    \n",
        "    #model = keras.models.load_model(\"s2s\")\n",
        "    \n",
        "    encoder_model,decoder_model = self.inference_model(model)\n",
        "    data_list = [[\"SNO\", \"Input Data\", \"Target Data\", \"Predicted Data\"]]\n",
        "\n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    for i in range(len(y_test)):##\n",
        "      #input_seq = val_encoder_input_data[i : i + 1]\n",
        "      input_seq = x_test[i : i + 1]\n",
        "      result = self.decode_sequence(encoder_model,decoder_model,input_seq)\n",
        "      #target = val_target_texts[i]\n",
        "      target = y_test[i]\n",
        "      target = target[1:len(target)-1]\n",
        "      result = result[0:len(result)-1]\n",
        "      #print(\"Target: %s \\n Result: %s\" % (target, result))\n",
        "      dlist = [i+1, test_input_texts[i], target, result]\n",
        "      data_list.append(dlist)\n",
        "\n",
        "      if result.strip() == target.strip():\n",
        "        global_correct = global_correct + 1\n",
        "      \n",
        "      global_total = global_total + 1\n",
        "      accuracy_epoch = global_correct/global_total\n",
        "    \n",
        "    with open('predictions_vanilla.csv', 'w', newline='') as file:\n",
        "      writer = csv.writer(file, delimiter='\\t')\n",
        "      writer.writerows(data_list)\n",
        "    val_accuracy = global_correct/global_total\n",
        "    print(val_accuracy)\n",
        "    \n",
        "  def inference_model(self,model):\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "      encoder_outputs, state_h_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output\n",
        "      encoder_states = [state_h_enc]\n",
        "      encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_1\n",
        "      decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "    elif self.cell_type == 'LSTM':\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output  # lstm_1\n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "      encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_1\n",
        "      decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec, state_c_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "\n",
        "    decoder_dense = model.get_layer('dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "  def decode_sequence(self,encoder_model,decoder_model,input_seq):\n",
        "\n",
        "    states_value = [encoder_model.predict(input_seq)] * self.num_dec \n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['B']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "          dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "          \n",
        "        elif self.cell_type == 'LSTM':  \n",
        "          dummy = decoder_model.predict([target_seq] + states_value)\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "        \n",
        "        #print(output_tokens[0,:,:])\n",
        "        if self.pred_type == 'greedy':\n",
        "          beam_w = 1\n",
        "        elif self.pred_type == 'beam_search':\n",
        "          beam_w = self.beam_width\n",
        "        sampled_token_index = self.beam_search_decoder(output_tokens[0,:,:], beam_w)\n",
        "        sampled_token_index = sampled_token_index[beam_w-1][0]\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit when the decoded sequence either hits max length\n",
        "        # or finds stop character\n",
        "        if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence\n",
        "  \n",
        "  def beam_search_decoder(self,data, k):\n",
        "    sequence = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_cands = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequence)):\n",
        "        seq, score = sequence[i]\n",
        "        for j in range(len(row)):\n",
        "          cand = [seq + [j], score - log(row[j])]\n",
        "          all_cands.append(cand)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_cands, key=lambda tup:tup[1])\n",
        "      sequence = ordered[:k]\n",
        "    return sequence"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JQuAVW1bniP"
      },
      "source": [
        "BEST Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbLTlTkoAHES"
      },
      "source": [
        "best_batch_size = 128\n",
        "best_beam_width = 5\n",
        "best_cell_type = 'LSTM'\n",
        "best_dec_search = 'greedy'\n",
        "best_dropout = 0.2\n",
        "best_epochs = 15\n",
        "best_hidden_size = 128\n",
        "best_in_emb = 128\n",
        "best_learning_rate = 0.001\n",
        "best_num_dec = 3\n",
        "best_num_enc = 2"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IbWbu-PzKab"
      },
      "source": [
        "Compile and fit model and get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E8zabQJwc47M",
        "outputId": "b70e1a73-428d-455d-e65e-029c83fb7dfb"
      },
      "source": [
        "model_rnn = MyRNN(cell_type = best_cell_type, in_emb = best_in_emb, hidden_size=best_hidden_size,\n",
        "                learning_rate= best_learning_rate, dropout=best_dropout,pred_type = best_dec_search,epochs = best_epochs,\n",
        "                batch_size = best_batch_size, beam_width = best_beam_width, num_enc = best_num_enc, num_dec = best_num_dec)\n",
        "  \n",
        "model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Enc_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Enc_emb (Embedding)             (None, None, 128)    3456        Enc_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Enc_hidden_1 (LSTM)             [(None, None, 128),  131584      Enc_emb[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Dec_emb (Embedding)             (None, None, 128)    6272        Dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Enc_hidden_2 (LSTM)             [(None, None, 128),  131584      Enc_hidden_1[0][0]               \n",
            "                                                                 Enc_hidden_1[0][1]               \n",
            "                                                                 Enc_hidden_1[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_1 (LSTM)             [(None, None, 128),  131584      Dec_emb[0][0]                    \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_2 (LSTM)             [(None, None, 128),  131584      Dec_hidden_1[0][0]               \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_3 (LSTM)             [(None, None, 128),  131584      Dec_hidden_2[0][0]               \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 49)     6321        Dec_hidden_3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 673,969\n",
            "Trainable params: 673,969\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n",
            "533/533 [==============================] - 28s 23ms/step - loss: 1.0044 - accuracy: 0.2747\n",
            "Epoch 2/15\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.4704 - accuracy: 0.6452\n",
            "Epoch 3/15\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.2609 - accuracy: 0.8076\n",
            "Epoch 4/15\n",
            " 34/533 [>.............................] - ETA: 11s - loss: 0.1918 - accuracy: 0.8639"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-7874c43de658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 batch_size = best_batch_size, beam_width = best_beam_width, num_enc = best_num_enc, num_dec = best_num_dec)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-ed3142ebbd7c>\u001b[0m in \u001b[0;36mbuild_fit\u001b[0;34m(self, encoder_input_data, decoder_input_data, decoder_target_data, x_test, y_test)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpRHzX_copMn"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictions_vanilla.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
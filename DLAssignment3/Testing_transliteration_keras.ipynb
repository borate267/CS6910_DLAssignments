{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Testing_transliteration_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNDodnmvkEb"
      },
      "source": [
        "The following code uses the best set of hyperparameters obtained after a set 87 sweeps, on the test data. The program outputs a predictions_vanilla.csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "#Importing essentials and Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcsn9UVpEyH",
        "outputId": "e9b44f38-85f6-49f4-dd6d-419c17818e57"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 13 07:02:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    32W / 250W |    841MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "import io\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from math import log1p \n",
        "import keras"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "**Fetching the dataset** \n",
        "\n",
        "Lexicons for Latin-Tamil are taken from Google's Dakshina dataset. The necessary datasets have been uploaded to github, cloned and used for the reminder of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "51e4e78d-d4d5-41a8-8d02-3f5e4422305f"
      },
      "source": [
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lexicon-dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "print_data = True #Set to False if you do not want to print "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "#Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzvL2e20Sx-"
      },
      "source": [
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCH31Lo6waWp"
      },
      "source": [
        "#Shuffling the datasets, creating a vocabulary and tokenising the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mC2ICFKRUZA",
        "outputId": "002dca7b-8cef-498a-e6b0-424ca992c923"
      },
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(test_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "# Holds the vocabulary\n",
        "source_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# Holds unshuffled datasets\n",
        "input_texts_ns = []\n",
        "target_texts_ns = []\n",
        "test_input_texts_ns = []\n",
        "test_target_texts_ns = []\n",
        "\n",
        "# The target words are appended with B and E which stand for\n",
        "# Beginning (start sequence character) and End (end sequence character) respectively\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_texts_ns.append(input_text)\n",
        "    target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in source_characters:\n",
        "            source_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(test_source, test_target):\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    test_input_texts_ns.append(input_text)\n",
        "    test_target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in source_characters:\n",
        "            source_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "# Shuffling the datasets\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_texts.append(input_texts_ns[arr[i]])\n",
        "    target_texts.append(target_texts_ns[arr[i]])\n",
        "\n",
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "for i in range(len(test_source)):\n",
        "    test_input_texts.append(test_input_texts_ns[arr1[i]])\n",
        "    test_target_texts.append(test_target_texts_ns[arr1[i]])\n",
        "\n",
        "# Adding the padding character\n",
        "source_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "\n",
        "# Creating the vocabulary\n",
        "source_characters = sorted(list(source_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Essential parameters which will be periodically used in the reminder of the code\n",
        "\n",
        "num_encoder_tokens = len(source_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "test_max_encoder_seq_length = max([len(txt) for txt in test_input_texts])\n",
        "test_max_decoder_seq_length = max([len(txt) for txt in test_target_texts])\n",
        "\n",
        "# Tokenising elements in the vocabulary\n",
        "source_token_index = dict([(char, i) for i, char in enumerate(source_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "reverse_source_char_index = dict((i, char) for char, i in source_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Print out some data\n",
        "if (print_data):\n",
        "  print(\"Number of training samples:\", len(input_texts))\n",
        "  print(\"Number of testing samples: \", len(test_source))\n",
        "  print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "  print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "  print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "  print(\"Max sequence length for test inputs:\", test_max_encoder_seq_length)\n",
        "  print(\"Max sequence length for test outputs:\", test_max_decoder_seq_length)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 68218\n",
            "Number of testing samples:  6864\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Max sequence length for test inputs: 23\n",
            "Max sequence length for test outputs: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3W77CTE4tp"
      },
      "source": [
        "Character Embedding\n",
        "\n",
        "**Encoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_encSeqLen)**\n",
        "\n",
        "**Decoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen)**\n",
        "\n",
        "**Decoder Target Sequences**: Padded to a maximum length of max_decSeqLen characters with a vocabulary of sizeofTamilVocab different characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen, sizeofTamilVocab)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3i7ykYbtQ2K"
      },
      "source": [
        "# For training\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = source_token_index[char]\n",
        "    encoder_input_data[i, t + 1 :] = source_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "# For testing\n",
        "test_encoder_input_data = np.zeros((len(input_texts), test_max_encoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_input_data = np.zeros((len(input_texts), test_max_decoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_target_data = np.zeros((len(input_texts), test_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_encoder_input_data[i, t] = source_token_index[char]\n",
        "    test_encoder_input_data[i, t + 1 :] = source_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        test_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    test_decoder_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    test_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui9pFxRvSdeT"
      },
      "source": [
        "#TESTING GROUND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7q6xNssrRwb"
      },
      "source": [
        "x_test = test_encoder_input_data\n",
        "y_test = test_target_texts\n",
        "#print(np.shape(y_test))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTFnlQ8NyU1"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvI1fI7UOPMT"
      },
      "source": [
        "class MyRNN(object):\n",
        "  def __init__(self,cell_type = 'RNN',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "               dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "               num_enc = 1,num_dec = 1):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.in_emb = in_emb\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.pred_type = pred_type\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.beam_width = beam_width\n",
        "    self.num_enc = num_enc\n",
        "    self.num_dec = num_dec\n",
        "\n",
        "  def build_fit(self,encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test):\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = Input(shape=(None, ),name = 'Enc_inputs')\n",
        "\n",
        "    # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and\n",
        "    # output embedding dimension of size in_enc.\n",
        "    enc_emb =  Embedding(num_encoder_tokens, self.in_emb , mask_zero = True,name = 'Enc_emb')(encoder_inputs)\n",
        "    encoder_outputs = enc_emb\n",
        "\n",
        "    # Adding num_enc number of cells of type LSTM/GRU/RNN\n",
        "    if self.cell_type == 'LSTM':\n",
        "      encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "      encoder_states = [state_h, state_c]\n",
        "\n",
        "      for i in range( 2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs,initial_state = encoder_states)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h = encoder_gru(encoder_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h = encoder_gru(encoder_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      encoder_outputs, state_h = encoder_rnn(encoder_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None,), name = 'Dec_inputs')\n",
        "    dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero = True, name = 'Dec_emb')\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    decoder_outputs = dec_emb\n",
        "\n",
        "    # Adding num_dec number of cells of type LSTM/GRU/RNN\n",
        "    if self.cell_type == 'LSTM':\n",
        "      decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state = encoder_states)\n",
        "      \n",
        "      for i in range(2, self.num_dec +1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _ = decoder_gru(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _ = decoder_gru(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      decoder_outputs, _ = decoder_rnn(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name = 'dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that takes encoder and decoder input \n",
        "    # to output decoder_outputs\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.epochs\n",
        "        )\n",
        "\n",
        "    #model.save(\"s2s\")\n",
        "    \n",
        "    #model = keras.models.load_model(\"s2s\")\n",
        "    \n",
        "    encoder_model,decoder_model = self.inference_model(model)\n",
        "    data_list = [[\"SNO\", \"Input Data\", \"Target Data\", \"Predicted Data\"]]\n",
        "\n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    for i in range(len(y_test)):##\n",
        "      #input_seq = val_encoder_input_data[i : i + 1]\n",
        "      input_seq = x_test[i : i + 1]\n",
        "      result = self.decode_sequence(encoder_model,decoder_model,input_seq)\n",
        "      #target = val_target_texts[i]\n",
        "      target = y_test[i]\n",
        "      target = target[1:len(target)-1]\n",
        "      result = result[0:len(result)-1]\n",
        "      #print(\"Target: %s \\n Result: %s\" % (target, result))\n",
        "      dlist = [i+1, test_input_texts[i], target, result]\n",
        "      data_list.append(dlist)\n",
        "\n",
        "      if result.strip() == target.strip():\n",
        "        global_correct = global_correct + 1\n",
        "      \n",
        "      global_total = global_total + 1\n",
        "      accuracy_epoch = global_correct/global_total\n",
        "    \n",
        "    with open('predictions_vanilla.tsv', 'w', newline='') as file:\n",
        "      writer = csv.writer(file, delimiter='\\t')\n",
        "      writer.writerows(data_list)\n",
        "    val_accuracy = global_correct/global_total\n",
        "    print(val_accuracy)\n",
        "    \n",
        "  def inference_model(self,model):\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "      encoder_outputs, state_h_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output\n",
        "      encoder_states = [state_h_enc]\n",
        "      encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_1\n",
        "      decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "    elif self.cell_type == 'LSTM':\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output  # lstm_1\n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "      encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_1\n",
        "      decoder_outputs = model.get_layer('Dec_emb')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec, state_c_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "\n",
        "    decoder_dense = model.get_layer('dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "  def decode_sequence(self,encoder_model,decoder_model,input_seq):\n",
        "\n",
        "    states_value = [encoder_model.predict(input_seq)] * self.num_dec \n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['B']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "          dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "          \n",
        "        elif self.cell_type == 'LSTM':  \n",
        "          dummy = decoder_model.predict([target_seq] + states_value)\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "        \n",
        "        #print(output_tokens[0,:,:])\n",
        "        if self.pred_type == 'greedy':\n",
        "          beam_w = 1\n",
        "        elif self.pred_type == 'beam_search':\n",
        "          beam_w = self.beam_width\n",
        "        sampled_token_index = self.beam_search_decoder(output_tokens[0,:,:], beam_w)\n",
        "        sampled_token_index = sampled_token_index[beam_w-1][0]\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit when the decoded sequence either hits max length\n",
        "        # or finds stop character\n",
        "        if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence\n",
        "  \n",
        "  def beam_search_decoder(self,data, k):\n",
        "    sequence = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_cands = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequence)):\n",
        "        seq, score = sequence[i]\n",
        "        for j in range(len(row)):\n",
        "          cand = [seq + [j], score - log(row[j])]\n",
        "          all_cands.append(cand)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_cands, key=lambda tup:tup[1])\n",
        "      sequence = ordered[:k]\n",
        "    return sequence"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JQuAVW1bniP"
      },
      "source": [
        "BEST Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbLTlTkoAHES"
      },
      "source": [
        "best_batch_size = 128\n",
        "best_beam_width = 5\n",
        "best_cell_type = 'LSTM'\n",
        "best_dec_search = 'greedy'\n",
        "best_dropout = 0.2\n",
        "best_epochs = 20\n",
        "best_hidden_size = 128\n",
        "best_in_emb = 128\n",
        "best_learning_rate = 0.001\n",
        "best_num_dec = 3\n",
        "best_num_enc = 2"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IbWbu-PzKab"
      },
      "source": [
        "Compile and fit model and get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8zabQJwc47M",
        "outputId": "561821ab-ca7b-4f24-e9c3-9e99726717b9"
      },
      "source": [
        "model_rnn = MyRNN(cell_type = best_cell_type, in_emb = best_in_emb, hidden_size=best_hidden_size,\n",
        "                learning_rate= best_learning_rate, dropout=best_dropout,pred_type = best_dec_search,epochs = best_epochs,\n",
        "                batch_size = best_batch_size, beam_width = best_beam_width, num_enc = best_num_enc, num_dec = best_num_dec)\n",
        "  \n",
        "model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Enc_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Enc_emb (Embedding)             (None, None, 128)    3456        Enc_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Enc_hidden_1 (LSTM)             [(None, None, 128),  131584      Enc_emb[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Dec_emb (Embedding)             (None, None, 128)    6272        Dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Enc_hidden_2 (LSTM)             [(None, None, 128),  131584      Enc_hidden_1[0][0]               \n",
            "                                                                 Enc_hidden_1[0][1]               \n",
            "                                                                 Enc_hidden_1[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_1 (LSTM)             [(None, None, 128),  131584      Dec_emb[0][0]                    \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_2 (LSTM)             [(None, None, 128),  131584      Dec_hidden_1[0][0]               \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "Dec_hidden_3 (LSTM)             [(None, None, 128),  131584      Dec_hidden_2[0][0]               \n",
            "                                                                 Enc_hidden_2[0][1]               \n",
            "                                                                 Enc_hidden_2[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 49)     6321        Dec_hidden_3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 673,969\n",
            "Trainable params: 673,969\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "533/533 [==============================] - 28s 22ms/step - loss: 1.0155 - accuracy: 0.2663\n",
            "Epoch 2/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.4680 - accuracy: 0.6456\n",
            "Epoch 3/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.2684 - accuracy: 0.8025\n",
            "Epoch 4/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.1876 - accuracy: 0.8660\n",
            "Epoch 5/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.1492 - accuracy: 0.8952\n",
            "Epoch 6/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.1249 - accuracy: 0.9119\n",
            "Epoch 7/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.1084 - accuracy: 0.9238\n",
            "Epoch 8/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0947 - accuracy: 0.9337\n",
            "Epoch 9/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0854 - accuracy: 0.9395\n",
            "Epoch 10/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0769 - accuracy: 0.9445\n",
            "Epoch 11/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0694 - accuracy: 0.9503\n",
            "Epoch 12/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0636 - accuracy: 0.9539\n",
            "Epoch 13/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.0584 - accuracy: 0.9574\n",
            "Epoch 14/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0544 - accuracy: 0.9602\n",
            "Epoch 15/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0504 - accuracy: 0.9625\n",
            "Epoch 16/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0468 - accuracy: 0.9652\n",
            "Epoch 17/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.0445 - accuracy: 0.9669\n",
            "Epoch 18/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0406 - accuracy: 0.9694\n",
            "Epoch 19/20\n",
            "533/533 [==============================] - 12s 23ms/step - loss: 0.0391 - accuracy: 0.9706\n",
            "Epoch 20/20\n",
            "533/533 [==============================] - 12s 22ms/step - loss: 0.0373 - accuracy: 0.9716\n",
            "0.4995629370629371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tpRHzX_copMn",
        "outputId": "0ef9d03e-56e7-4112-e2f4-a411bcd49d0e"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictions_vanilla.tsv\")\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_89f481e8-0849-46d1-9bba-05668151eb49\", \"predictions_vanilla.tsv\", 472357)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
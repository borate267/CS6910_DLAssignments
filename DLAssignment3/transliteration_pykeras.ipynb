{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transliteration_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "# Essentials\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import io\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhpFjBqec5sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2adcbac-9b04-4073-f798-f2b13eb46028"
      },
      "source": [
        "# Fetching the dataset\n",
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lexicon-dataset'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "96c823d0-126b-4f18-c50e-4b026eecaee4"
      },
      "source": [
        "# Reading the dataset\n",
        "\n",
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "dev_dir = \"lexicon-dataset/ta.translit.sampled.dev.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_input, train_target = read_corpus(train_dir)\n",
        "valid_input, valid_target = read_corpus(dev_dir)\n",
        "test_input, test_target = read_corpus(test_dir)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_input))\n",
        "print(\"Number of validation samples: \", len(valid_input))\n",
        "print(\"Number of testing samples: \", len(test_input))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  68218\n",
            "Number of validation samples:  6827\n",
            "Number of testing samples:  6864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhYt6cWVwwrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052c6917-6356-4bed-acbb-32641afbcd93"
      },
      "source": [
        "# PRE-PROCESSING\n",
        "\n",
        "#### Appending decoder inputs with <bow> and <eow>\n",
        "\n",
        "#bow = \"<bow>\"\n",
        "#eow = \"<eow>\"\n",
        "\n",
        "bow = 0\n",
        "eow = 1\n",
        "\n",
        "#decoder_inputs = [bow + text + eow for text in train_target] \n",
        "\n",
        "#### Creating vocabularies for the dataset\n",
        "\n",
        "vocab_tamil = set()\n",
        "vocab_latin = set()\n",
        "\n",
        "for i in range(len(train_input)):\n",
        "  input_str = train_input[i].lower()\n",
        "  for char in input_str:\n",
        "    if char not in vocab_latin:\n",
        "      vocab_latin.add(char)\n",
        "\n",
        "for i in range(len(train_target)):\n",
        "  input_str = train_target[i] \n",
        "  for char in input_str:\n",
        "    if char not in vocab_tamil:\n",
        "      vocab_tamil.add(char)\n",
        "#vocab_tamil.add('<bow>')\n",
        "#vocab_tamil.add('<eow>')\n",
        "\n",
        "vocab_tamil = sorted(list(vocab_tamil))\n",
        "vocab_latin = sorted(list(vocab_latin))\n",
        "sizeofTamilVocab = len(vocab_tamil)\n",
        "sizeofLatinVocab = len(vocab_latin)\n",
        "max_encSeqLen = max([len(sample) for sample in train_input])\n",
        "max_decSeqLen = max([len(sample) for sample in train_target])\n",
        "\n",
        "#print(\"Tamil vocabulary: \", vocab_tamil)\n",
        "#print(\"Latin vocabulary: \", vocab_latin)\n",
        "#print(\"Size of Tamil vocabulary: \", sizeofTamilVocab)\n",
        "#print(\"Size of Latin vocabulary: \", sizeofLatinVocab)\n",
        "#print(\"Maximum length of encoder size: \", max_encSeqLen)\n",
        "#print(\"Maximum length of decoder size: \", max_decSeqLen)\n",
        "\n",
        "#### Tokenising the encoder and decoder inputs\n",
        "\n",
        "latin_token_index = dict([(char, i) for i, char in enumerate(vocab_latin)])\n",
        "tamil_token_index = dict([(char, i) for i, char in enumerate(vocab_tamil)])\n",
        "print(tamil_token_index)\n",
        "#### Convert sequences of characters to sequences of tokens\n",
        "\n",
        "encoder_seq = []\n",
        "decoder_seq = []\n",
        "\n",
        "for i in range(len(train_input)):\n",
        "  input_str = train_input[i].lower()\n",
        "  dummy = []\n",
        "  for char in input_str:\n",
        "    if char in vocab_latin:\n",
        "      dummy.append(latin_token_index[char])\n",
        "  encoder_seq.append(dummy)\n",
        "#print(latin_token_index)\n",
        "#print(encoder_seq[0:2])\n",
        "\n",
        "for i in range(len(decoder_inputs)):\n",
        "  input_str = decoder_inputs[i]\n",
        "  dummy = []\n",
        "  for char in input_str:\n",
        "    if char in vocab_tamil:\n",
        "      dummy.append(tamil_token_index[char])\n",
        "  decoder_seq.append(dummy)\n",
        "#print(tamil_token_index)\n",
        "#print(decoder_seq[0:2])\n",
        "\n",
        "#### Padding sequences that are to be fed as input to the encoder and decoder RNN\n",
        "\n",
        "encoder_inputdata = pad_sequences(encoder_seq, maxlen= max_encSeqLen, dtype='int32', padding='post', truncating='post')\n",
        "decoder_inputdata = pad_sequences(decoder_seq, maxlen= max_decSeqLen, dtype='int32', padding='post', truncating='post')\n",
        "#print(encoder_inputdata[0])\n",
        "#print(decoder_inputdata[0])\n",
        "\n",
        "#### CHARACTER EMBEDDING\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ஃ': 0, 'அ': 1, 'ஆ': 2, 'இ': 3, 'ஈ': 4, 'உ': 5, 'ஊ': 6, 'எ': 7, 'ஏ': 8, 'ஐ': 9, 'ஒ': 10, 'ஓ': 11, 'க': 12, 'ங': 13, 'ச': 14, 'ஜ': 15, 'ஞ': 16, 'ட': 17, 'ண': 18, 'த': 19, 'ந': 20, 'ன': 21, 'ப': 22, 'ம': 23, 'ய': 24, 'ர': 25, 'ற': 26, 'ல': 27, 'ள': 28, 'ழ': 29, 'வ': 30, 'ஷ': 31, 'ஸ': 32, 'ஹ': 33, 'ா': 34, 'ி': 35, 'ீ': 36, 'ு': 37, 'ூ': 38, 'ெ': 39, 'ே': 40, 'ை': 41, 'ொ': 42, 'ோ': 43, 'ௌ': 44, '்': 45}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDUkr7crrCFG"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "input_emb_size = 256 #Input embedding size\n",
        "num_enc = 128 #Number of encoder layers\n",
        "num_dec = 128 #Number of decoder layers\n",
        "hidden_size = 16 #Hidden layer size\n",
        "cell_type = 'RNN' #Cell type\n",
        "dropout = 0.3 #Dropout\n",
        "batch_size = 16 #batch size\n",
        "\n",
        "#TODO: Beam search in decoder, add more hyperparas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lusIQaNa3vn"
      },
      "source": [
        "# seq2seq model architecture\n",
        "\n",
        "# Input embedding layer : A feedforward layer of size sizeofLatinVocab x input_emb_size\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEAAmuove36L"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-D0RjT2dfC9"
      },
      "source": [
        "# check that the function works as expected\n",
        "#test_seq = np.array([[3, 5, 1]])\n",
        "test_seq = encoder_inputdata[0]\n",
        "one_hot = one_hot_encode(test_seq, sizeofLatinVocab)\n",
        "print(test_seq)\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwNy4vqIdpaq"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4g1QzsgdqYj"
      },
      "source": [
        "batches = get_batches(encoder_inputdata, batch_size, sizeofLatinVocab)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wwGXlE5dxB0"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TWEFTY7l7R0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc20eYAjFRi8"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "#from torchtext import data\n",
        "from torchtext.legacy import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876filWRsuXg"
      },
      "source": [
        "class Encoder_RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, embbed_size, layers_n,model,dropout_prob):\n",
        "    super(Encoder_RNN, self).__init__()\n",
        "      \n",
        "    self.input_size = input_size\n",
        "    self.embbed_size = embbed_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.layers_n = layers_n\n",
        "    self.model = model\n",
        "    self.dropout_prob = dropout_prob\n",
        "\n",
        "    #initialize the embedding layer with input and embbed dimention\n",
        "    self.embedding = nn.Embedding(input_size, self.embbed_size)\n",
        "       \n",
        "    if model == \"RNN\":\n",
        "      self.rnn = nn.RNN(self.embbed_size, self.hidden_size, num_layers=self.layers_n, batch_first=True)\n",
        "    elif model == \"LSTM\":\n",
        "      self.lstm = nn.LSTM(self.embbed_size, self.hidden_size, num_layers=self.layers_n, batch_first=True)\n",
        "    elif model == \"GRU\":\n",
        "      self.gru = nn.GRU(self.embbed_size, self.hidden_size, num_layers=self.layers_n)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "  def forward(self, input):\n",
        "      \n",
        "       embedded = self.embedding(input).view(1,1,-1)\n",
        "       if model == \"RNN\":\n",
        "          outputs, hidden = self.rnn(embedded)\n",
        "       elif model == \"LSTM\":\n",
        "          outputs, hidden = self.lstm(embedded)\n",
        "       elif model == \"GRU\":\n",
        "          outputs, hidden = self.gru(embedded)\n",
        "\n",
        "       return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ySRRbXguKcX"
      },
      "source": [
        "class Decoder_RNN(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_size, embbed_size, layers_n,model,dropout_prob):\n",
        "       super(Decoder_RNN, self).__init__()\n",
        "\n",
        "       self.embbed_size = embbed_size\n",
        "       self.hidden_size = hidden_size\n",
        "       self.output_dim = output_dim\n",
        "       self.layers_n = layers_n\n",
        "       self.model = model\n",
        "       self.dropout_prob = dropout_prob\n",
        "\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_size)\n",
        "\n",
        "       if model == \"RNN\":\n",
        "         self.rnn = nn.RNN(self.embbed_size, self.hidden_size, num_layers=self.layers_n, batch_first=True)\n",
        "       if model == \"LSTM\":\n",
        "         self.lstm = nn.LSTM(self.embbed_size, self.hidden_size, num_layers=self.layers_n, batch_first=True)\n",
        "       elif model == \"GRU\":\n",
        "         self.gru = nn.GRU(self.embbed_size, self.hidden_size, num_layers=self.layers_n)\n",
        "       \n",
        "       self.out = nn.Linear(self.hidden_size, output_dim)\n",
        "       self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "       self.dropout = nn.Dropout(dropout_prob)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       if model == \"RNN\":\n",
        "           output, hidden = self.rnn(embedded, hidden)\n",
        "       if model == \"LSTM\":\n",
        "           output, hidden = self.lstm(embedded, hidden)                    \n",
        "       elif model == \"GRU\":\n",
        "           output, hidden = self.gru(embedded, hidden)       \n",
        "       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "    \n",
        "       return prediction, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZV6pavEqrAW"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device):\n",
        "       super().__init__()\n",
        "      \n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "      #initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "       #encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "      #use the encoder’s hidden layer as the decoder hidden\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "  \n",
        "      #add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([bow], device=device)\n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "       return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZcQtOc14NQi"
      },
      "source": [
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  epoch = epoch\n",
        "  model = Seq2Seq(bla bla bla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gmBCt3n0Ud0"
      },
      "source": [
        "def train():\n",
        "  for epoch in range(1,epoch+1)\n",
        "\n",
        "    model.train()\n",
        "    if train_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(num_iteration)]\n",
        "  \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for iter in range(1, epoch+1):\n",
        "      \n",
        "      training_pair = training_pairs[iter - 1]\n",
        "      input_tensor = training_pair[0]\n",
        "      target_tensor = training_pair[1]\n",
        "\n",
        "      model_optimizer.zero_grad()\n",
        "\n",
        "      input_length = input_tensor.size(0)\n",
        "      \n",
        "      # print(input_tensor.shape)\n",
        "\n",
        "      output = model(input_tensor, target_tensor)\n",
        "\n",
        "      num_iter = output.size(0)\n",
        "      print(num_iter)\n",
        "      loss = 0\n",
        "\n",
        "#calculate the loss from a predicted sentence with the expected result\n",
        "      for ot in range(num_iter):\n",
        "        loss += criterion(output[ot], target_tensor[ot])\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      opt.step()\n",
        "      \n",
        "      model_optimizer.step()\n",
        "      epoch_loss = loss.item() / num_iter\n",
        "\n",
        "      total_loss_iterations += epoch_loss\n",
        "\n",
        "      return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aUXrd-87PIB"
      },
      "source": [
        "def train():\n",
        "  for epoch in range(1,epoch+1):\n",
        "    train_loss = 0\n",
        "    val_loss = 0 \n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1,epoch+1):\n",
        "\n",
        "      training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(num_iteration)]\n",
        "\n",
        "                     \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3haF0D9FFfen",
        "outputId": "57acdaa7-f806-444b-e335-f0124d15f4fa"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 26 04:22:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc20eYAjFRi8"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "#from torchtext import data\n",
        "from torchtext.legacy import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAKv728cfFrd"
      },
      "source": [
        "device = torch.device(\n",
        "  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d_6S20dF7nD",
        "outputId": "d21a86f8-eac9-4409-89d1-6c68b89c30c1"
      },
      "source": [
        "!pip install wget\n",
        "import wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=49cbc927789ddddbf8998ef80fde76ca036387cb113e83056eb1685851566561\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WHT23NdDGXhu",
        "outputId": "4ee4726d-09a3-4857-9c9a-1bb534e7eab8"
      },
      "source": [
        "wget.download('https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dakshina_dataset_v1.0.tar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUc_g4XNG1aN"
      },
      "source": [
        "!tar -xvf  '/content/dakshina_dataset_v1.0.tar' -C '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIJJTvM8HcLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c732abab-c3aa-4efb-8cd6-f8fe77674ed0"
      },
      "source": [
        "# Training using \"tamil\" language dataset\n",
        "# clone dakshina_dataset_v1.0/ta/lexicons/\n",
        "# Folders with training, test data:\n",
        "\"\"\"\n",
        "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
        "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv  - WHAT IS THIS???\n",
        "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\\ndakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv  - WHAT IS THIS???\\ndakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BNEZGKTF2q"
      },
      "source": [
        "# create Field objects\n",
        "tamil_words = data.Field()\n",
        "latin_words = data.Field()\n",
        "\n",
        "# create tuples representing the columns\n",
        "fields = [\n",
        "  ('tamil_words', tamil_words),\n",
        "  ('latin_words', latin_words),\n",
        "  (None, None), # ignore last column\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6dZ-eRse4D_"
      },
      "source": [
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "   path = '/content/dakshina_dataset_v1.0/ta/lexicons',\n",
        "   train = 'ta.translit.sampled.train.tsv',\n",
        "   validation = 'ta.translit.sampled.dev.tsv',\n",
        "   test = 'ta.translit.sampled.test.tsv',\n",
        "   format = 'tsv',\n",
        "   fields = fields,\n",
        "   skip_header = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGpAxdx7fMOX",
        "outputId": "9b184022-7b68-4365-ed26-6054994d72d2"
      },
      "source": [
        "import random\n",
        "#print(vars(train_data[11]))\n",
        "for ii in range(10):\n",
        "  a = random.randint(0, 100)\n",
        "  print(vars(train_data[a]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tamil_words': ['அகற்றக்'], 'latin_words': ['agattrak']}\n",
            "{'tamil_words': ['அக'], 'latin_words': ['aka']}\n",
            "{'tamil_words': ['அகச்சிவப்பு'], 'latin_words': ['akachchivappu']}\n",
            "{'tamil_words': ['அக'], 'latin_words': ['aka']}\n",
            "{'tamil_words': ['அகப்பொருள்'], 'latin_words': ['agapporul']}\n",
            "{'tamil_words': ['அகற்றப்பட'], 'latin_words': ['akatrtrapada']}\n",
            "{'tamil_words': ['அகதியாக'], 'latin_words': ['akathiyaaka']}\n",
            "{'tamil_words': ['அகன்ற'], 'latin_words': ['agantra']}\n",
            "{'tamil_words': ['ஃபோர்ஸ்'], 'latin_words': ['phoars']}\n",
            "{'tamil_words': ['ஃபேஸ்புக்'], 'latin_words': ['paespuk']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtQPY5buT2tP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "f8aec31f-2442-4922-f7b6-9a1e8ccc5cc4"
      },
      "source": [
        "tamil_words.build_vocab(train_data)\n",
        "latin_words.build_vocab(train_data)\n",
        "#print(len(tamil_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-402ecc1e45f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtamil_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlatin_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtamil_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tamil_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3muUbijlVJL1"
      },
      "source": [
        "# create iterators for train/valid/test datasets\n",
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "  (train_data, valid_data, test_data),\n",
        "  sort_key = lambda x: x.tamil_words,\n",
        "  sort = True,\n",
        "  batch_size = 32,\n",
        "  device = device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876filWRsuXg"
      },
      "source": [
        "class Encoder_RNN(nn.Module):\n",
        "   def __init__(self, input_size, hidden_size, embbed_size, layers_n,model,dropout_prob):\n",
        "       super(Encoder_RNN, self).__init__()\n",
        "      \n",
        "       self.input_size = input_size\n",
        "       self.embbed_size = embbed_size\n",
        "       self.hidden_size = hidden_size\n",
        "       self.layers_n = layers_n\n",
        "       self.model = model\n",
        "       self.dropout_prob = dropout_prob\n",
        "\n",
        "       #initialize the embedding layer with input and embbed dimention\n",
        "       self.embedding = nn.Embedding(input_size, self.embbed_size)\n",
        "       \n",
        "       if model == \"LSTM\":\n",
        "          self.lstm = nn.LSTM(self.embbed_size, self.hidden_size, layers_n=self.layers_n, batch_first=True)\n",
        "       \n",
        "       elif model == \"GRU\":\n",
        "          self.gru = nn.GRU(self.embbed_size, self.hidden_size, layers_n=self.layers_n)\n",
        "\n",
        "       self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "   def forward(self, input):\n",
        "      \n",
        "       embedded = self.embedding(input).view(1,1,-1)\n",
        "       if model == \"LSTM\":\n",
        "          outputs, hidden = self.gru(embedded)\n",
        "       elif model == \"GRU\":\n",
        "          outputs, hidden = self.lstm(embedded)\n",
        "\n",
        "       return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ySRRbXguKcX"
      },
      "source": [
        "class Decoder_RNN(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_size, embbed_size, layers_n,model,dropout_prob):\n",
        "       super(Decoder_RNN, self).__init__()\n",
        "\n",
        "       self.embbed_size = embbed_size\n",
        "       self.hidden_size = hidden_size\n",
        "       self.output_dim = output_dim\n",
        "       self.layers_n = layers_n\n",
        "       self.model = model\n",
        "       self.dropout_prob = dropout_prob\n",
        "\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_size)\n",
        "\n",
        "       if model == \"LSTM\":\n",
        "           self.lstm = nn.LSTM(self.embbed_size, self.hidden_size, layers_n=self.layers_n, batch_first=True)\n",
        "       elif model == \"GRU\":\n",
        "            self.gru = nn.GRU(self.embbed_size, self.hidden_size, layers_n=self.layers_n)\n",
        "       \n",
        "       self.out = nn.Linear(self.hidden_size, output_dim)\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "       self.dropout = nn.Dropout(dropout_prob)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       if model == \"LSTM\":\n",
        "           output, hidden = self.lstm(embedded, hidden)                    \n",
        "       elif model == \"GRU\":\n",
        "           output, hidden = self.gru(embedded, hidden)       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "    \n",
        "       return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
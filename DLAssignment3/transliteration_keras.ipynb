{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transliteration_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "# Essentials\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import io\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhpFjBqec5sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f68b640-8f91-44d3-90b7-116fcc70f533"
      },
      "source": [
        "# Fetching the dataset\n",
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lexicon-dataset'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "97bc371f-655a-4397-9b00-cd57fbca35d8"
      },
      "source": [
        "# Reading the dataset\n",
        "\n",
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "dev_dir = \"lexicon-dataset/ta.translit.sampled.dev.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_input, train_target = read_corpus(train_dir)\n",
        "valid_input, valid_target = read_corpus(dev_dir)\n",
        "test_input, test_target = read_corpus(test_dir)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_input))\n",
        "print(\"Number of validation samples: \", len(valid_input))\n",
        "print(\"Number of testing samples: \", len(test_input))\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  68218\n",
            "Number of validation samples:  6827\n",
            "Number of testing samples:  6864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhYt6cWVwwrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fce2efa-f948-4750-8e70-2236b6e23d58"
      },
      "source": [
        "# PRE-PROCESSING\n",
        "\n",
        "#### Appending decoder inputs with <bow> and <eow>\n",
        "\n",
        "bow = \"<bow>\"\n",
        "eow = \"<bos>\"\n",
        "decoder_inputs = [bow + text + eow for text in train_target] \n",
        "\n",
        "#### Creating vocabularies for the dataset\n",
        "\n",
        "vocab_tamil = set()\n",
        "vocab_latin = set()\n",
        "\n",
        "for i in range(len(train_input)):\n",
        "  input_str = train_input[i].lower()\n",
        "  for char in input_str:\n",
        "    if char not in vocab_latin:\n",
        "      vocab_latin.add(char)\n",
        "\n",
        "for i in range(len(train_target)):\n",
        "  input_str = train_target[i] \n",
        "  for char in input_str:\n",
        "    if char not in vocab_tamil:\n",
        "      vocab_tamil.add(char)\n",
        "vocab_tamil.add('<bow>')\n",
        "vocab_tamil.add('<eow>')\n",
        "\n",
        "vocab_tamil = sorted(list(vocab_tamil))\n",
        "vocab_latin = sorted(list(vocab_latin))\n",
        "sizeofTamilVocab = len(vocab_tamil)\n",
        "sizeofLatinVocab = len(vocab_latin)\n",
        "max_encSeqLen = max([len(sample) for sample in train_input])\n",
        "max_decSeqLen = max([len(sample) for sample in train_target])\n",
        "\n",
        "#print(\"Tamil vocabulary: \", vocab_tamil)\n",
        "#print(\"Latin vocabulary: \", vocab_latin)\n",
        "#print(\"Size of Tamil vocabulary: \", sizeofTamilVocab)\n",
        "#print(\"Size of Latin vocabulary: \", sizeofLatinVocab)\n",
        "#print(\"Maximum length of encoder size: \", max_encSeqLen)\n",
        "#print(\"Maximum length of decoder size: \", max_decSeqLen)\n",
        "\n",
        "#### Tokenising the encoder and decoder inputs\n",
        "\n",
        "latin_token_index = dict([(char, i) for i, char in enumerate(vocab_latin)])\n",
        "tamil_token_index = dict([(char, i) for i, char in enumerate(vocab_tamil)])\n",
        "  \n",
        "#### Convert sequences of characters to sequences of tokens\n",
        "\n",
        "encoder_seq = []\n",
        "decoder_seq = []\n",
        "\n",
        "for i in range(len(train_input)):\n",
        "  input_str = train_input[i].lower()\n",
        "  dummy = []\n",
        "  for char in input_str:\n",
        "    if char in vocab_latin:\n",
        "      dummy.append(latin_token_index[char])\n",
        "  encoder_seq.append(dummy)\n",
        "#print(latin_token_index)\n",
        "#print(encoder_seq[0:2])\n",
        "\n",
        "for i in range(len(decoder_inputs)):\n",
        "  input_str = decoder_inputs[i]\n",
        "  dummy = []\n",
        "  for char in input_str:\n",
        "    if char in vocab_tamil:\n",
        "      dummy.append(tamil_token_index[char])\n",
        "  decoder_seq.append(dummy)\n",
        "#print(tamil_token_index)\n",
        "#print(decoder_seq[0:2])\n",
        "\n",
        "#### Padding sequences that are to be fed as input to the encoder and decoder RNN\n",
        "\n",
        "encoder_inputdata = pad_sequences(encoder_seq, maxlen= max_encSeqLen, dtype='int32', padding='post', truncating='post')\n",
        "decoder_inputdata = pad_sequences(decoder_seq, maxlen= max_decSeqLen, dtype='int32', padding='post', truncating='post')\n",
        "#print(encoder_inputdata[0])\n",
        "#print(decoder_inputdata[0])\n",
        "\n",
        "#### CHARACTER EMBEDDING\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5  8  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0]\n",
            "[ 2 24 37 26 19 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPySuSYcNNji",
        "outputId": "471bc989-3a7e-4157-bec4-97f5ba45126c"
      },
      "source": [
        ""
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5  8  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDUkr7crrCFG"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "input_emb_size = 256 #Input embedding size\n",
        "num_enc = 128 #Number of encoder layers\n",
        "num_dec = 128 #Number of decoder layers\n",
        "hidden_size = 16 #Hidden layer size\n",
        "cell_type = 'RNN' #Cell type\n",
        "dropout = 0.3 #Dropout\n",
        "\n",
        "#TODO: Beam search in decoder, add more hyperparas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lusIQaNa3vn"
      },
      "source": [
        "# seq2seq model architecture\n",
        "\n",
        "# Input embedding layer : A feedforward layer of size sizeofLatinVocab x input_emb_size\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_transliteration_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "Essentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NsYPEEpM5cU"
      },
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "**Fetching the dataset** \n",
        "\n",
        "Lexicons for Latin-Tamil are taken from Google's Dakshina dataset. The necessary datasets have been uploaded to github, cloned and used for the reminder of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "a232cf1c-4729-4701-c35e-75aadeefd809"
      },
      "source": [
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lexicon-dataset'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "print_data = False"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "30835ea6-6746-4f13-a4ab-3cb96839f1ad"
      },
      "source": [
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "dev_dir = \"lexicon-dataset/ta.translit.sampled.dev.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "valid_source, valid_target = read_corpus(dev_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_source))\n",
        "print(\"Number of validation samples: \", len(valid_source))\n",
        "print(\"Number of testing samples: \", len(test_source))\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  68218\n",
            "Number of validation samples:  6827\n",
            "Number of testing samples:  6864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSRmOgVXKRCM"
      },
      "source": [
        "# Pre-processing data\n",
        "\n",
        "# The following lists contain the source and target words that are to\n",
        "# be used for training and validation\n",
        "\n",
        "train_source = []\n",
        "train_target = []\n",
        "val_source = []\n",
        "val_target = []\n",
        "\n",
        "# The following entities hold the vocabulary for Tamil and latin languages\n",
        "vocab_source = set()\n",
        "vocab_target = set()\n",
        "\n",
        "# Procuring training data\n",
        "with io.open(train_dir, encoding ='utf-8') as f:\n",
        "  for line in f:\n",
        "    if '\\t' not in line:\n",
        "      continue\n",
        "    tokens = line.rstrip().split(\"\\t\")\n",
        "    input_text = tokens[1]\n",
        "    target_text = tokens[0]\n",
        "    train_source.append(input_text)\n",
        "    train_target.append(target_text)\n",
        "\n",
        "    # Creating vocabulary\n",
        "    for char in input_text:\n",
        "        if char not in vocab_source:\n",
        "            vocab_source.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in vocab_target:\n",
        "            vocab_target.add(char)\n",
        "\n",
        "# Procuring Validation data\n",
        "with io.open(dev_dir, encoding ='utf-8') as f:\n",
        "  for line in f:\n",
        "    if '\\t' not in line:\n",
        "      continue\n",
        "    tokens = line.rstrip().split(\"\\t\")\n",
        "    input_text = tokens[1]\n",
        "    target_text = tokens[0]\n",
        "    val_source.append(input_text)\n",
        "    val_target.append(target_text)\n",
        "\n",
        "    # Updating vocabulary\n",
        "    for char in input_text:\n",
        "        if char not in vocab_source:\n",
        "            vocab_source.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in vocab_target:\n",
        "            vocab_target.add(char)\n",
        "\n",
        "vocab_source = sorted(list(vocab_source))\n",
        "vocab_target = sorted(list(vocab_target))\n",
        "num_encoder_tokens = len(vocab_source)\n",
        "num_decoder_tokens = len(vocab_target)\n",
        "max_encoder_seq_length = max([len(txt) for txt in train_source])\n",
        "max_decoder_seq_length = max([len(txt) for txt in train_target])\n",
        "\n",
        "if (print_data):\n",
        "    print(\"Number of samples:\", len(train_source))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# Creating tokens for vocabulary\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(vocab_source)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(vocab_target)])\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3W77CTE4tp"
      },
      "source": [
        "Character Embedding\n",
        "\n",
        "**Encoded Source Sequences**: Padded to a maximum length of max_encSeqLen characters with a vocabulary of sizeofLatinVocab different characters. \n",
        "**SHAPE: (len(train_source), max_encSeqLen, sizeofLatinVocab)**\n",
        "\n",
        "**Encoded Target Sequences**: Padded to a maximum length of max_decSeqLen characters with a vocabulary of sizeofTamilVocab different characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen, sizeofTamilVocab)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9P14Os9q_YE"
      },
      "source": [
        "def generate_batch(X, Y):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_encoder_seq_length),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_decoder_seq_length),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], Y[j:j+batch_size])):\n",
        "                for t, char in enumerate(input_text):\n",
        "                    encoder_input_data[i, t] = input_token_index[char]\n",
        "                for t, char in enumerate(target_text):\n",
        "                    decoder_input_data[i, t] = target_token_index[char]\n",
        "                    if t > 0:\n",
        "                        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "             "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNou1PDNIwb"
      },
      "source": [
        "Configuring the Sweep Hyperparameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6v4kKgONH4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usWeF1s4Cvpq"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 10  # Number of epochs to train for.\n",
        "latent_dim = 128  # Latent dimensionality of the encoding space."
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTFnlQ8NyU1"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z3ixC5zHe-b",
        "outputId": "792bf5f2-6683-4359-f059-e4acc82438f3"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, ))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim , mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that takes encoder and decoder input \n",
        "# to output decoder_outputs\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 128)    3328        input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 128)    5888        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_18 (LSTM)                  [(None, 128), (None, 131584      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_19 (LSTM)                  [(None, None, 128),  131584      embedding_5[0][0]                \n",
            "                                                                 lstm_18[0][1]                    \n",
            "                                                                 lstm_18[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, None, 46)     5934        lstm_19[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 278,318\n",
            "Trainable params: 278,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6vOLyJdImHP"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3JP4ppGI74h",
        "outputId": "0edf941c-8951-40cb-c9b8-b9af38f59808"
      },
      "source": [
        "history=model.fit_generator(generator = generate_batch(train_source, train_target),\n",
        "                    steps_per_epoch = len(train_source)//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(val_source, val_target),\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1065/1065 [==============================] - ETA: 0s - loss: 0.7693 - acc: 0.2665"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of transliteration_final_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f65c41a5c55848ae9f840662793f5fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9746f39ea516405fbba6d72a5e5e1c3a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbe72a91fd644b2ca267c916dd9d1e84",
              "IPY_MODEL_35f5d4a347c94c2388f3e87546f7bce9"
            ]
          }
        },
        "9746f39ea516405fbba6d72a5e5e1c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbe72a91fd644b2ca267c916dd9d1e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_e5fea947d21245f299f3ee7bbe1360f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.08MB of 0.08MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c6618d373f745a7a1f4541b6aad8b76"
          }
        },
        "35f5d4a347c94c2388f3e87546f7bce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bebec0a55e63463687d5bff5aac654e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aab5f5f2168d4ebf918a70d795b5ff70"
          }
        },
        "e5fea947d21245f299f3ee7bbe1360f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c6618d373f745a7a1f4541b6aad8b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bebec0a55e63463687d5bff5aac654e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aab5f5f2168d4ebf918a70d795b5ff70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "Essentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcsn9UVpEyH",
        "outputId": "ac58b775-62c1-4d36-c1c4-20e1df06a76e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May  6 17:04:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NsYPEEpM5cU"
      },
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "**Fetching the dataset** \n",
        "\n",
        "Lexicons for Latin-Tamil are taken from Google's Dakshina dataset. The necessary datasets have been uploaded to github, cloned and used for the reminder of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "2077123a-2512-4364-f448-09ad40db0730"
      },
      "source": [
        "!git clone https://github.com/borate267/lexicon-dataset.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lexicon-dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "print_data = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzvL2e20Sx-"
      },
      "source": [
        "train_dir = \"lexicon-dataset/ta.translit.sampled.train.tsv\"\n",
        "dev_dir = \"lexicon-dataset/ta.translit.sampled.dev.tsv\"\n",
        "test_dir = \"lexicon-dataset/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "# The following function reads the raw text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "valid_source, valid_target = read_corpus(dev_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mC2ICFKRUZA",
        "outputId": "b31686ed-6075-4585-edd4-4d3c4d9f031a"
      },
      "source": [
        "# Pre-processing data\n",
        "\n",
        "go = 'G'\n",
        "stop = 'S'\n",
        "\n",
        "# The following lists contain the source and target words that are to\n",
        "# be used for training and validation\n",
        "\n",
        "train_source = []\n",
        "train_target = []\n",
        "val_source = []\n",
        "val_target = []\n",
        "\n",
        "# The following entities hold the vocabulary for Tamil and latin languages\n",
        "vocab_source = set()\n",
        "vocab_target = set()\n",
        "\n",
        "# Procuring training data\n",
        "with io.open(train_dir, encoding ='utf-8') as f:\n",
        "  for line in f:\n",
        "    if '\\t' not in line:\n",
        "      continue\n",
        "    tokens = line.rstrip().split(\"\\t\")\n",
        "    input_text = tokens[1]\n",
        "    target_text = tokens[0]\n",
        "    train_source.append(input_text)\n",
        "    train_target.append(go + target_text + stop)\n",
        "\n",
        "    # Creating vocabulary\n",
        "    for char in input_text:\n",
        "        if char not in vocab_source:\n",
        "            vocab_source.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in vocab_target:\n",
        "            vocab_target.add(char)\n",
        "\n",
        "vocab_target.add(go)\n",
        "vocab_target.add(stop)\n",
        "\n",
        "# Procuring Validation data\n",
        "with io.open(dev_dir, encoding ='utf-8') as f:\n",
        "  for line in f:\n",
        "    if '\\t' not in line:\n",
        "      continue\n",
        "    tokens = line.rstrip().split(\"\\t\")\n",
        "    input_text = tokens[1]\n",
        "    target_text = tokens[0]\n",
        "    val_source.append(input_text)\n",
        "    val_target.append(go + target_text + stop)\n",
        "\n",
        "    # Updating vocabulary\n",
        "    for char in input_text:\n",
        "        if char not in vocab_source:\n",
        "            vocab_source.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in vocab_target:\n",
        "            vocab_target.add(char)\n",
        "\n",
        "vocab_source = sorted(list(vocab_source))\n",
        "vocab_target = sorted(list(vocab_target))\n",
        "num_encoder_tokens = len(vocab_source)\n",
        "num_decoder_tokens = len(vocab_target)\n",
        "max_encoder_seq_length = max([len(txt) for txt in train_source])\n",
        "max_decoder_seq_length = max([len(txt) for txt in train_target])\n",
        "\n",
        "if (print_data):\n",
        "    print(\"Number of training samples: \", len(train_source))\n",
        "    print(\"Number of validation samples: \", len(valid_source))\n",
        "    print(\"Number of testing samples: \", len(test_source))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# Creating tokens for vocabulary\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(vocab_source)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(vocab_target)])\n",
        "\n",
        "#(TODO 1): Check if the start and end of word have tokens and check if they are in vocab\n",
        "print(train_target[0])\n",
        "print(target_token_index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples:  68218\n",
            "Number of validation samples:  6827\n",
            "Number of testing samples:  6864\n",
            "Number of unique input tokens: 26\n",
            "Number of unique output tokens: 48\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "Gஃபியட்S\n",
            "{'G': 0, 'S': 1, 'ஃ': 2, 'அ': 3, 'ஆ': 4, 'இ': 5, 'ஈ': 6, 'உ': 7, 'ஊ': 8, 'எ': 9, 'ஏ': 10, 'ஐ': 11, 'ஒ': 12, 'ஓ': 13, 'க': 14, 'ங': 15, 'ச': 16, 'ஜ': 17, 'ஞ': 18, 'ட': 19, 'ண': 20, 'த': 21, 'ந': 22, 'ன': 23, 'ப': 24, 'ம': 25, 'ய': 26, 'ர': 27, 'ற': 28, 'ல': 29, 'ள': 30, 'ழ': 31, 'வ': 32, 'ஷ': 33, 'ஸ': 34, 'ஹ': 35, 'ா': 36, 'ி': 37, 'ீ': 38, 'ு': 39, 'ூ': 40, 'ெ': 41, 'ே': 42, 'ை': 43, 'ொ': 44, 'ோ': 45, 'ௌ': 46, '்': 47}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3W77CTE4tp"
      },
      "source": [
        "Character Embedding\n",
        "\n",
        "**Encoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_encSeqLen)**\n",
        "\n",
        "**Decoder Input Sequences**: Padded to a maximum length of max_encSeqLen characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen)**\n",
        "\n",
        "**Decoder Target Sequences**: Padded to a maximum length of max_decSeqLen characters with a vocabulary of sizeofTamilVocab different characters. \n",
        "**SHAPE: (len(train_source), max_decSeqLen, sizeofTamilVocab)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwPNb93PRwHT"
      },
      "source": [
        "For training :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQW089M5R0ES"
      },
      "source": [
        "encoder_input_data = np.zeros((len(train_source), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(train_source), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(train_source), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(train_source, train_target)):\n",
        "\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(target_text):\n",
        "\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        if t < len(target_text) - 1: \n",
        "            # we only consider until before the end character. \n",
        "            decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui9pFxRvSdeT"
      },
      "source": [
        "For Validation and testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrpkSzdITkey"
      },
      "source": [
        "val_max_encoder_seq_length = max([len(txt) for txt in val_source])\n",
        "val_max_decoder_seq_length = max([len(txt) for txt in val_target])\n",
        "\n",
        "val_encoder_input_data = np.zeros((len(val_source), val_max_encoder_seq_length), dtype=\"float32\")\n",
        "val_decoder_input_data = np.zeros((len(val_source), val_max_decoder_seq_length), dtype=\"float32\")\n",
        "val_decoder_target_data = np.zeros((len(val_source), val_max_decoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_source, val_target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "\n",
        "        if t < len(target_text) - 1:\n",
        "            # we only consider until before the end character.\n",
        "            val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1] = target_token_index[char]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVGsHbTJn807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ec0df4-1b35-4a5f-d009-90f6d30a68d2"
      },
      "source": [
        "#(TODO 2): Verify decoder input and target have an offset of one\n",
        "print(val_decoder_input_data[26])\n",
        "print(val_decoder_target_data[26])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  3. 16. 37. 23. 47.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.]\n",
            "[ 3. 16. 37. 23. 47.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7q6xNssrRwb"
      },
      "source": [
        "x_test = [val_encoder_input_data, val_decoder_input_data]\n",
        "y_test = val_decoder_target_data"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNou1PDNIwb"
      },
      "source": [
        "Configuring the Sweep Hyperparameter dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTFnlQ8NyU1"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvI1fI7UOPMT"
      },
      "source": [
        "class MyRNN(object):\n",
        "  def __init__(self,cell_type = 'RNN',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "               dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "               num_enc_dec =2):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.in_emb = in_emb\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.pred_type = pred_type\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.beam_width = beam_width\n",
        "    self.num_enc_dec = num_enc_dec\n",
        "\n",
        "  def build_fit(self,encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test):\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "    # Add an Embedding layer expecting input vocab of size num_encoder_tokens, and\n",
        "    # output embedding dimension of size in_enc.\n",
        "    enc_emb =  Embedding(num_encoder_tokens, self.in_emb , mask_zero = True)(encoder_inputs)\n",
        "\n",
        "    encoder_outputs = enc_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "        # Add a LSTM layer with hidden_size internal units.\n",
        "        if self.num_enc_dec == 1:\n",
        "          encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_1\")\n",
        "          encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
        "          encoder_states = [state_h, state_c]\n",
        "        \n",
        "        elif self.num_enc_dec == 2:\n",
        "          encoder_lstm_1 = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_1\")\n",
        "          encoder_outputs_1, state_h_1, state_c_1 = encoder_lstm_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1, state_c_1]\n",
        "\n",
        "          encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_2\")\n",
        "          encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs_1)\n",
        "          encoder_states = [state_h, state_c]\n",
        "          \n",
        "        elif self.num_enc_dec == 3:\n",
        "          encoder_lstm_1 = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_1\")\n",
        "          encoder_outputs_1, state_h_1, state_c_1 = encoder_lstm_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1, state_c_1]\n",
        "          \n",
        "          encoder_lstm_2 = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_2\")\n",
        "          encoder_outputs_2, state_h_2, state_c_2 = encoder_lstm_2(encoder_outputs_1)\n",
        "          encoder_states_2 = [state_h_2, state_c_2]\n",
        "          \n",
        "          encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_LSTM_3\")\n",
        "          encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs_2)\n",
        "          encoder_states = [state_h, state_c]\n",
        "    \n",
        "    elif self.cell_type == 'GRU':\n",
        "        if self.num_enc_dec == 1:\n",
        "          encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_1\")\n",
        "          encoder_outputs, state_h = encoder_gru(encoder_outputs)\n",
        "          encoder_states = [state_h]\n",
        "        \n",
        "        elif self.num_enc_dec == 2:\n",
        "          encoder_gru_1 = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_\")\n",
        "          encoder_outputs_1, state_h_1 = encoder_gru_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1]\n",
        "\n",
        "          encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_2\")\n",
        "          encoder_outputs, state_h = encoder_gru(encoder_outputs_1)\n",
        "          encoder_states = [state_h]\n",
        "          \n",
        "        elif self.num_enc_dec == 3:\n",
        "          encoder_gru_1 = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_1\")\n",
        "          encoder_outputs_1,state_h_1 = encoder_gru_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1]\n",
        "          \n",
        "          encoder_gru_2 = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_2\")\n",
        "          encoder_outputs_2, state_h_2 = encoder_gru_2(encoder_outputs_1)\n",
        "          encoder_states_2 = [state_h_2]\n",
        "          \n",
        "          encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_GRU_3\")\n",
        "          encoder_outputs, state_h = encoder_gru(encoder_outputs_2)\n",
        "          encoder_states = [state_h]\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "        if self.num_enc_dec == 1:\n",
        "          encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_1\")\n",
        "          encoder_outputs, state_h = encoder_rnn(encoder_outputs)\n",
        "          encoder_states = [state_h]\n",
        "        \n",
        "        elif self.num_enc_dec == 2:\n",
        "          encoder_rnn_1 = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_\")\n",
        "          encoder_outputs_1, state_h_1 = encoder_rnn_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1]\n",
        "\n",
        "          encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_2\")\n",
        "          encoder_outputs, state_h = encoder_rnn(encoder_outputs_1)\n",
        "          encoder_states = [state_h]\n",
        "          \n",
        "        elif self.num_enc_dec == 3:\n",
        "          encoder_rnn_1 = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_1\")\n",
        "          encoder_outputs_1,state_h_1 = encoder_rnn_1(encoder_outputs)\n",
        "          encoder_states_1 = [state_h_1]\n",
        "          \n",
        "          encoder_rnn_2 = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_2\")\n",
        "          encoder_outputs_2, state_h_2 = encoder_rnn_2(encoder_outputs_1)\n",
        "          encoder_states_2 = [state_h_2]\n",
        "          \n",
        "          encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_RNN_3\")\n",
        "          encoder_outputs, state_h = encoder_rnn(encoder_outputs_2)\n",
        "          encoder_states = [state_h]\n",
        "\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_emb_layer = Embedding(num_decoder_tokens, self.hidden_size, mask_zero = True)\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_outputs = dec_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "    \n",
        "      if self.num_enc_dec == 1:\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_1\")\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      elif self.num_enc_dec == 2:\n",
        "        decoder_lstm_1 = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_1\")\n",
        "        decoder_outputs_1, _, _ = decoder_lstm_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_2\")\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs_1, initial_state = encoder_states)\n",
        "      \n",
        "      elif self.num_enc_dec == 3:\n",
        "        decoder_lstm_1 = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_1\")\n",
        "        decoder_outputs_1, _, _ = decoder_lstm_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_lstm_2 = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_2\")\n",
        "        decoder_outputs_2, _, _ = decoder_lstm_2(decoder_outputs, initial_state = encoder_states_2)\n",
        "\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_LSTM_3\")\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_outputs_2, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "\n",
        "      if self.num_enc_dec == 1:\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_1\")\n",
        "        decoder_outputs, _ = decoder_gru(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      elif self.num_enc_dec == 2:\n",
        "        decoder_gru_1 = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_1\")\n",
        "        decoder_outputs_1, _ = decoder_gru_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_2\")\n",
        "        decoder_outputs, _ = decoder_gru(decoder_outputs_1, initial_state = encoder_states)\n",
        "      \n",
        "      elif self.num_enc_dec == 3:\n",
        "        decoder_gru_1 = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_1\")\n",
        "        decoder_outputs_1, _ = decoder_gru_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_gru_2 = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_2\")\n",
        "        decoder_outputs_2, _ = decoder_gru_2(decoder_outputs, initial_state = encoder_states_2)\n",
        "\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_GRU_3\")\n",
        "        decoder_outputs, _ = decoder_gru(decoder_outputs_2, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "\n",
        "      if self.num_enc_dec == 1:\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_1\")\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_outputs, initial_state = encoder_states)\n",
        "\n",
        "      elif self.num_enc_dec == 2:\n",
        "        decoder_rnn_1 = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_1\")\n",
        "        decoder_outputs_1, _ = decoder_rnn_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_2\")\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_outputs_1, initial_state = encoder_states)\n",
        "      \n",
        "      elif self.num_enc_dec == 3:\n",
        "        decoder_rnn_1 = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_1\")\n",
        "        decoder_outputs_1, _ = decoder_rnn_1(decoder_outputs, initial_state = encoder_states_1)\n",
        "\n",
        "        decoder_rnn_2 = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_2\")\n",
        "        decoder_outputs_2, _ = decoder_rnn_2(decoder_outputs, initial_state = encoder_states_2)\n",
        "\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_RNN_3\")\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_outputs_2, initial_state = encoder_states)\n",
        "\n",
        "    hidden = Dense(128, activation=\"relu\")\n",
        "    hidden_outputs = hidden(decoder_outputs)\n",
        "    drop = Dropout(self.dropout)\n",
        "    dropout_out = drop(hidden_outputs)\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(dropout_out)\n",
        "\n",
        "    # Define the model that takes encoder and decoder input \n",
        "    # to output decoder_outputs\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "  \n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.epochs,\n",
        "        callbacks = [WandbCallback()]\n",
        "        )\n",
        "    \n",
        "    model.save(\"seq2seq\")\n",
        "    \n",
        "    output = model.predict(x_test, batch_size = self.batch_size)\n",
        "    \n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    \n",
        "    for i in range(len(y_test)):\n",
        "      local_correct = 0\n",
        "      arr = y_test[i]\n",
        "      true_arr = arr[np.nonzero(arr)]\n",
        "      true_length = len(true_arr)\n",
        "      \n",
        "      if self.pred_type == 'greedy':\n",
        "        beam_width = 1\n",
        "        result = self.beam_search_decoder(output[i,:,:], beam_width)\n",
        "        result = result[0][0]\n",
        "\n",
        "      elif self.pred_type == 'beam_search':  \n",
        "        beam_width = self.beam_width  \n",
        "        result = self.beam_search_decoder(output[i,:,:], beam_width)\n",
        "        result = result[self.beam_width-1][0]  \n",
        "      \n",
        "      pred_arr = result[:true_length-1]\n",
        "      true_arr = true_arr[:true_length-1]\n",
        "\n",
        "      for i in range(len(pred_arr)):\n",
        "        if true_arr[i] == pred_arr[i]:\n",
        "          local_correct = local_correct + 1\n",
        "          global_total = global_total + 1\n",
        "        else:\n",
        "          global_total = global_total + 1\n",
        "      \n",
        "      global_correct = global_correct + local_correct\n",
        "      \n",
        "    val_accuracy = global_correct/global_total\n",
        "    print(val_accuracy)\n",
        "\n",
        "    wandb.log({'val_accuracy' : val_accuracy})\n",
        "\n",
        "  def beam_search_decoder(self,data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpH2nJGdCcZ0"
      },
      "source": [
        "Sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6v4kKgONH4q"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'in_emb': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'num_enc_dec': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['RNN', 'GRU', 'LSTM']\n",
        "        },\n",
        "        'dec_search': {\n",
        "            'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [5,10]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC0C9Ol8QcW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6dc853d1-d373-4b74-8e2b-4bbb72d85fbb"
      },
      "source": [
        "# Initialize a new sweep\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"cs6910assignment3\", project=\"RNN\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 4lok3n9a\n",
            "Sweep URL: https://wandb.ai/cs6910assignment3/RNN/sweeps/4lok3n9a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QeYGKXmlKHr"
      },
      "source": [
        "def train_sweep():\n",
        "  config_defaults = {\n",
        "        'dropout': 0.4,\n",
        "        'learning_rate': 1e-3,\n",
        "        'batch_size': 32,\n",
        "        'epochs' : 10,\n",
        "        'in_emb': 32,\n",
        "        'num_enc_dec': 2,\n",
        "        'hidden_size': 32,\n",
        "        'cell_type': 'RNN',\n",
        "        'dec_search': 'beam_search',\n",
        "        'beam_width': 5\n",
        "        }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(config = config_defaults)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  wandb.run.name = 'cell_type_'+ str(config.cell_type)+'_dec_search_'+ config.dec_search+'_bs_'+str(config.batch_size)\n",
        "  \n",
        "  model_rnn = MyRNN(cell_type = config.cell_type, in_emb = config.in_emb, hidden_size=config.hidden_size,\n",
        "                learning_rate= config.learning_rate, dropout=config.dropout,pred_type = config.dec_search,epochs = config.epochs,\n",
        "                batch_size = config.batch_size, beam_width = config.beam_width, num_enc_dec = config.num_enc_dec)\n",
        "  \n",
        "  model_rnn.build_fit(encoder_input_data,decoder_input_data,decoder_target_data,x_test, y_test)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcuc8Q2CB-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f65c41a5c55848ae9f840662793f5fc1",
            "9746f39ea516405fbba6d72a5e5e1c3a",
            "cbe72a91fd644b2ca267c916dd9d1e84",
            "35f5d4a347c94c2388f3e87546f7bce9",
            "e5fea947d21245f299f3ee7bbe1360f7",
            "2c6618d373f745a7a1f4541b6aad8b76",
            "bebec0a55e63463687d5bff5aac654e8",
            "aab5f5f2168d4ebf918a70d795b5ff70"
          ]
        },
        "outputId": "553dc71f-aea1-4465-ac2b-42b6c28d528a"
      },
      "source": [
        "wandb.agent(sweep_id, train_sweep,count=100)\n",
        "#wandb.agent(\"4lok3n9a\", entity=\"cs6910assignment3\",project=\"RNN\", function =train,count=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mu3wqjjc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: beam_search\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc_dec: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.29<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">breezy-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs6910assignment3/RNN\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/cs6910assignment3/RNN/sweeps/4lok3n9a\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN/sweeps/4lok3n9a</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/cs6910assignment3/RNN/runs/mu3wqjjc\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN/runs/mu3wqjjc</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210506_173649-mu3wqjjc</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 64)     1664        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 64)     3072        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Enc_GRU_1 (GRU)                 [(None, None, 64), ( 24960       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Dec_GRU_1 (GRU)                 [(None, None, 64), ( 24960       embedding_1[0][0]                \n",
            "                                                                 Enc_GRU_1[0][1]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 128)    8320        Dec_GRU_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 128)    0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 48)     6192        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 69,168\n",
            "Trainable params: 69,168\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "533/533 [==============================] - 67s 112ms/step - loss: 1.0935 - accuracy: 0.1390\n",
            "Epoch 2/10\n",
            "533/533 [==============================] - 60s 112ms/step - loss: 0.8092 - accuracy: 0.2998\n",
            "Epoch 3/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.7122 - accuracy: 0.3467\n",
            "Epoch 4/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6838 - accuracy: 0.3630\n",
            "Epoch 5/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6610 - accuracy: 0.3756\n",
            "Epoch 6/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6481 - accuracy: 0.3897\n",
            "Epoch 7/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6328 - accuracy: 0.4040\n",
            "Epoch 8/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6203 - accuracy: 0.4189\n",
            "Epoch 9/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.6062 - accuracy: 0.4301\n",
            "Epoch 10/10\n",
            "533/533 [==============================] - 60s 113ms/step - loss: 0.5942 - accuracy: 0.4425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: seq2seq/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: seq2seq/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.35883778102397834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2956<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65c41a5c55848ae9f840662793f5fc1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210506_173649-mu3wqjjc/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210506_173649-mu3wqjjc/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.59036</td></tr><tr><td>accuracy</td><td>0.44563</td></tr><tr><td>_runtime</td><td>646</td></tr><tr><td>_timestamp</td><td>1620323255</td></tr><tr><td>_step</td><td>10</td></tr><tr><td>val_accuracy</td><td>0.35884</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>accuracy</td><td>▁▅▅▆▆▇▇▇██</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>val_accuracy</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-sweep-4</strong>: <a href=\"https://wandb.ai/cs6910assignment3/RNN/runs/mu3wqjjc\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN/runs/mu3wqjjc</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9nfo375n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: beam_search\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc_dec: 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.29<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">amber-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs6910assignment3/RNN\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/cs6910assignment3/RNN/sweeps/4lok3n9a\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN/sweeps/4lok3n9a</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/cs6910assignment3/RNN/runs/9nfo375n\" target=\"_blank\">https://wandb.ai/cs6910assignment3/RNN/runs/9nfo375n</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210506_174748-9nfo375n</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 128)    3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 64)     3072        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Enc_GRU_ (GRU)                  [(None, None, 64), ( 37248       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Dec_GRU_1 (GRU)                 [(None, None, 64), ( 24960       embedding_1[0][0]                \n",
            "                                                                 Enc_GRU_[0][1]                   \n",
            "__________________________________________________________________________________________________\n",
            "Enc_GRU_2 (GRU)                 [(None, None, 64), ( 24960       Enc_GRU_[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Dec_GRU_2 (GRU)                 [(None, None, 64), ( 24960       Dec_GRU_1[0][0]                  \n",
            "                                                                 Enc_GRU_2[0][1]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 128)    8320        Dec_GRU_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 128)    0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 48)     6192        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 133,040\n",
            "Trainable params: 133,040\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            " 717/2132 [=========>....................] - ETA: 4:43 - loss: 0.8379 - accuracy: 0.2788"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
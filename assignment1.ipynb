{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_AFnIQzYIeK"
      },
      "source": [
        "\"\"\" The following code implements gradient descent and its variants with backpropogation for an image classification problem\n",
        "\n",
        "Created by Bharati. K EE20D700\n",
        "\"\"\"\n",
        "\n",
        "# WandB â€“ Install the W&B library\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9FaDXktYQj7"
      },
      "source": [
        "# Essentials\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pylab as pl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BItNyinfYVCj"
      },
      "source": [
        "#Fetch the dataset and visualise the images\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "wandb.init(project=\"cs6910assignment1\", entity=\"bharatik\")\n",
        "\n",
        "#Define the labels for the images that are being detected\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Load the training data \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Split data for cross validation\n",
        "x_val = x_train[54000:]\n",
        "y_val = y_train[54000:]\n",
        "x_train = x_train[:54000]\n",
        "y_train = y_train[:54000]  \n",
        "\n",
        "image = [];\n",
        "label = [];\n",
        "for i in range(54000):\n",
        "  if len(label) >= 10:\n",
        "    break;\n",
        "  if class_names[y_train[i]] not in label:\n",
        "      image.append(x_train[i])\n",
        "      label.append(class_names[y_train[i]])\n",
        "#wandb.log({\"Examples\": [ wandb.Image(img, caption=caption) for img, caption in zip(image,label)]})\n",
        "\n",
        "# Vectorise and normalize the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_val  = x_val.reshape(x_val.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_val  = x_val / 255.0\n",
        "\n",
        "# One hot encoding for labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_val   = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cRWUZa_2EGJ"
      },
      "source": [
        "# Defining the Neural network and optimisers\n",
        "\n",
        "# The following function initialises the weights and biases for a neural network\n",
        "# that has (number_hidden + 1) layers.\n",
        "def init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size ):\n",
        "    size = [] # this contains the list of number of nodes of every layer in the network\n",
        "    for j in range(number_hidden):\n",
        "      size.append(hid_input_size)\n",
        "    size = [net_input_size] + size + [net_output_size]\n",
        "    #print(size)\n",
        "    theta0 = {}\n",
        "\n",
        "    # Reference: (1) https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
        "    #            (2) https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
        "\n",
        "    if weight_init == 'xavier':\n",
        "      if act_func != 'relu': #xavier weight initialisation for sigmoid and tanh\n",
        "        for i in range(1, number_hidden+2):\n",
        "          lower = -1/ np.sqrt(size[i-1])\n",
        "          upper = 1/ np.sqrt(size[i-1])\n",
        "          theta0[\"W\" + str(i)] = lower + np.random.randn(size[i],size[i-1])*(upper- lower)\n",
        "          theta0[\"b\" + str(i)] = lower + np.random.randn(size[i],1)*(upper - lower)\n",
        "\n",
        "      else: #He Weight Initialization for reLU activation\n",
        "      # generate random number and multiply by a standard deviation that is equal to sqrt(2/number of nodes in previous layer)\n",
        "        for i in range(1, number_hidden+2): \n",
        "          theta0[\"W\" + str(i)] = np.random.randn(size[i], size[i-1])*(np.sqrt(2/(size[i-1])))\n",
        "          theta0[\"b\" + str(i)] = np.random.randn(size[i], 1)*(np.sqrt(2/(size[i-1])))\n",
        "\n",
        "    else: #random initialisation\n",
        "       for i in range(1, number_hidden+2):\n",
        "          if act_func != 'relu':\n",
        "            theta0[\"W\" + str(i)] = np.random.randn(size[i], size[i-1])\n",
        "            theta0[\"b\" + str(i)] = np.random.randn(size[i], 1)\n",
        "          else:\n",
        "            theta0[\"W\" + str(i)] = np.random.randn(size[i], size[i-1])*(np.sqrt(2/(size[i]+size[i-1])))\n",
        "            theta0[\"b\" + str(i)] = np.zeros((size[i],1))\n",
        "\n",
        "    return theta0\n",
        "\n",
        "# The following functions are used for activation at the hidden layers\n",
        "def sigmoid(x):\n",
        "\t  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return (x>0)*(x) #+ 0.01*((x<0)*x)\n",
        "\n",
        "def tanh(x):\n",
        "\t  return np.tanh(x)\n",
        "   \n",
        "def activation(x, act_func):\n",
        "    if act_func == \"sigmoid\":\n",
        "        return sigmoid(x)\n",
        "    elif act_func == \"tanh\":\n",
        "        return tanh(x)\n",
        "    elif act_func == \"relu\":\n",
        "        return relu(x)\n",
        "\n",
        "# This function is used for activation at the output layer\n",
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "\n",
        "# The following functions return the gradients of the various activation functions \n",
        "def grad_sigmoid(x):\n",
        "    return (sigmoid(x))*(1 - sigmoid(x))\n",
        "\n",
        "def grad_relu(x): # unit step\n",
        "    return (x>0)*(np.ones(np.shape(x))) #+ (x<0)*(0.01*np.ones(np.shape(x)))\n",
        "\n",
        "def grad_tanh(x):\n",
        "    return (1 - (tanh(x))**2)\n",
        "\n",
        "# The following functions are used to calculate loss\n",
        "def squared_loss(x, y, sum_norm, regpara):\n",
        "    loss = np.mean((y-x)**2) + regpara/2*(sum_norm)\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss(X, Y, sum_norm, regpara):\n",
        "    x = np.array(X).reshape(-1)\n",
        "    y = np.array(Y).reshape(-1)\n",
        "    logx = np.log(x)\n",
        "    loss_vec = (-1)*(y*logx)\n",
        "    loss = np.sum(loss_vec) + regpara/2*(sum_norm)\n",
        "    return loss    \n",
        "\n",
        "\n",
        "# Forward Propogation\n",
        "def forward_propogation(x, act_func, theta, number_hidden):\n",
        "    a = {} # dictionary of pre-activation values\n",
        "    h = {} # dictionary of activation values\n",
        "    # converting to a 2-dimensional array\n",
        "    if x.ndim == 1:\n",
        "      x = np.expand_dims(x, axis=1)\n",
        "    h = {\"h0\": x} \n",
        "\n",
        "    for i in range(1, number_hidden + 2):\n",
        "      \n",
        "      if i < number_hidden + 1:\n",
        "\n",
        "        # computing pre-activation values\n",
        "        W_current = theta[\"W\" + str(i)]\n",
        "        b_current = theta[\"b\" + str(i)]\n",
        "        h_previous = h[\"h\" + str(i-1)]\n",
        "        a_current = np.dot(W_current,h_previous) + b_current\n",
        "        a[\"a\" + str(i)] = a_current\n",
        "\n",
        "        # computing activation values\n",
        "        h_current = activation(a_current, act_func)\n",
        "        h[\"h\" + str(i)] = h_current  \n",
        "\n",
        "      else: # for output layer\n",
        "\n",
        "        a_current = np.dot(theta[\"W\" + str(i)],h[\"h\" + str(i-1)]) + theta[\"b\" + str(i)]\n",
        "        h_current = softmax(a_current)  \n",
        "        h[\"h\" + str(i)] = h_current  \n",
        "\n",
        "    y_hat = h[\"h\" + str(number_hidden + 1)]\n",
        "\n",
        "    return h, a, y_hat\n",
        "\n",
        "\n",
        "# Initialising the gradients for backpropagation. The gradients are computed wrt\n",
        "# output units, hidden layers, weights and biases\n",
        "def init_grad(number_hidden, hid_input_size, net_input_size, net_output_size ):\n",
        "    size = [] # this contains the list of number of nodes of every layer in the network\n",
        "    for j in range(number_hidden):\n",
        "        size.append(hid_input_size)\n",
        "    size = [net_input_size] + size + [net_output_size]\n",
        "\n",
        "    grad = {} \n",
        "    grad = { \"dh0\": np.zeros((net_input_size,1)), \"da0\": np.zeros((net_input_size,1)) } # is used for back prop computation\n",
        "    \n",
        "    for i in range(1, number_hidden + 2):\n",
        "        grad[\"da\" + str(i)] = np.zeros((size[i],1))\n",
        "        grad[\"dh\" + str(i)] = np.zeros((size[i],1))\n",
        "        grad[\"dW\" + str(i)] = np.zeros((size[i],size[i-1]))\n",
        "        grad[\"db\" + str(i)] = np.zeros((size[i],1))\n",
        "\n",
        "    return grad\n",
        "\n",
        "# The following function implements Backpropogation as described in Lecture 4\n",
        "\n",
        "def backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara):\n",
        "\n",
        "    grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size ) \n",
        "    \n",
        "    a[\"a0\"] = np.zeros((net_input_size,1)) \n",
        "    \n",
        "    # Computing gradient wrt output layer\n",
        "    if y.ndim == 1:\n",
        "      y = np.expand_dims(y, axis=1) # Converting to a 2-dimensional array\n",
        "      \n",
        "    if loss_func == \"cross\":\n",
        "        grad[\"da\" + str(number_hidden + 1)] = -1*(y - y_hat) #Enctrain label is one hot encoded\n",
        "    else:\n",
        "        grad[\"da\" + str(number_hidden + 1)] = -1*(y - y_hat)*y_hat - y_hat*(np.dot((y_hat - y).T, y_hat))\n",
        "    \n",
        "    for i in range(number_hidden + 1 ,0,-1): # L to 1\n",
        "\n",
        "      # Computing gradients wrt parameters\n",
        "      grad[\"dW\" + str(i)] = np.dot( grad[\"da\" + str(i)], (h[\"h\" + str(i-1)]).T ) + regpara*(theta[\"W\" + str(i)])\n",
        "      grad[\"db\" + str(i)] = grad[\"da\" + str(i)]\n",
        "\n",
        "      # Computing gradients wrt layer below\n",
        "      grad[\"dh\" + str(i-1)] = np.dot( (theta[\"W\" + str(i)]).T, grad[\"da\" + str(i)] )\n",
        "\n",
        "      # Computing gradients wrt layer below (preactivation)\n",
        "      if act_func == \"sigmoid\":\n",
        "          grad[\"da\" + str(i-1)] = (grad[\"dh\" + str(i-1)])*(grad_sigmoid(a[\"a\" + str(i-1)]))      \n",
        "      elif act_func == \"tanh\":\n",
        "          grad[\"da\" + str(i-1)] = (grad[\"dh\" + str(i-1)])*(grad_tanh(a[\"a\" + str(i-1)]))\n",
        "      elif act_func == \"relu\":\n",
        "          grad[\"da\" + str(i-1)] = (grad[\"dh\" + str(i-1)])*(grad_relu(a[\"a\" + str(i-1)]))\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "# The function computes accuracies and errors for training and validation\n",
        "\n",
        "def loss_accuracy_compute(ind, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara):\n",
        "\n",
        "    sum_norm = 0\n",
        "    for i in range(1,number_hidden + 2):\n",
        "        sum_norm = sum_norm + np.sum(np.square(theta[\"W\"+str(i)]))\n",
        "\n",
        "    train_loss = 0 # training loss\n",
        "    val_loss = 0 # validation loss\n",
        "    ctr = 0\n",
        "    ctr1 = 0\n",
        "\n",
        "    for j in range(0,54000):\n",
        "        x = x_train[j,:]\n",
        "        y = y_train[j,:]\n",
        "        h,a,y_hat = forward_propogation(x, act_func, theta, number_hidden)\n",
        "        \n",
        "      \n",
        "        if loss_func == \"cross\":\n",
        "            train_loss = train_loss + (cross_entropy_loss(y_hat,y,sum_norm, regpara))\n",
        "        else:\n",
        "            train_loss = train_loss + (squared_loss(y, y_hat,sum_norm, regpara))\n",
        "        \n",
        "        \n",
        "        # convert one hot encoded vector to label \n",
        "        y = np.argmax(y, axis = 0)\n",
        "        y_hat = np.argmax(y_hat, axis = 0)\n",
        "\n",
        "        if y == y_hat:\n",
        "          ctr = ctr + 1\n",
        "\n",
        "        if j < 6000:\n",
        "            x_vals = x_val[j,:]\n",
        "            y_vals = y_val[j,:]\n",
        "            h,a,y_hat_val = forward_propogation(x_vals, act_func, theta, number_hidden)\n",
        "          \n",
        "            if loss_func == \"cross\":\n",
        "                val_loss = val_loss + (cross_entropy_loss(y_hat_val,y_vals,sum_norm, regpara))\n",
        "            else:\n",
        "                val_loss = val_loss + (squared_loss(y_vals, y_hat_val,sum_norm, regpara))\n",
        "\n",
        "            # convert one hot encoded vector to label \n",
        "            y_vals = np.argmax(y_vals, axis = 0)\n",
        "            y_hat_val = np.argmax(y_hat_val, axis = 0)\n",
        "\n",
        "            if y_vals == y_hat_val:\n",
        "              ctr1 = ctr1 + 1\n",
        "\n",
        "\n",
        "    train_acc = ctr/54000.0\n",
        "    train_loss = train_loss/54000.0\n",
        "    val_acc = ctr1/6000.0\n",
        "    val_loss = val_loss/6000.0\n",
        "    max_accuracy.append([val_acc])\n",
        "\n",
        "    #wandb.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\":val_loss})\n",
        "    \n",
        "    \"\"\"\n",
        "    if (ind + 1) == num_epochs:\n",
        "      max_acc = np.max(max_accuracy)\n",
        "      wandb.log({\"accuracy\": max_acc })\n",
        "      \"\"\"\n",
        "    return val_acc, val_loss, train_acc, train_loss\n",
        "\n",
        "    \n",
        "\n",
        "# Vanilla Gradient Descent Algorithm (Lecture 4)\n",
        "def gradient_descent(num_epochs , number_hidden , hid_input_size , act_func , loss_func , weight_init, eta, regpara):\n",
        "  # Initialise paramaters\n",
        "  theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size ) \n",
        "  max_accuracy = []\n",
        "\n",
        "  # Loop\n",
        "  for i in range(num_epochs):\n",
        "\n",
        "    grads = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    arr  = np.arange(10)\n",
        "    np.random.shuffle(arr)\n",
        "\n",
        "    for j in range(0,10):\n",
        "      ind = arr[j]\n",
        "      x = x_train[ind,:]\n",
        "      y = y_train[ind,:]\n",
        "      h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)  \n",
        "      grad_current = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "      for key in grads:\n",
        "        grads[key] = grads[key] + grad_current[key]\n",
        "\n",
        "    for keynew in theta:\n",
        "      theta[keynew] = theta[keynew] - eta*(grads[\"d\" + keynew]) \n",
        "    \n",
        "    loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "\n",
        "  #return theta\n",
        "\n",
        "# Q3: Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "# The following functions creates the momenta for weights and biases to be used for \n",
        "# updating in the GD algorithm\n",
        "def init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size):\n",
        "    size = [] # this contains the list of number of nodes of every layer in the network\n",
        "    for j in range(number_hidden):\n",
        "      size.append(hid_input_size)\n",
        "    size = [net_input_size] + size + [net_output_size]\n",
        "    momenta = {}\n",
        "    for i in range(1, number_hidden+2):\n",
        "        momenta[\"mW\" + str(i)] = np.zeros((size[i], size[i-1]))\n",
        "        momenta[\"mb\" + str(i)] = np.zeros((size[i],1))\n",
        "    return momenta\n",
        "\n",
        "# The following function implements momentum based gradient descent (Lecture 5)\n",
        "def momentum_based_gradient_descent(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, gamma ):\n",
        "    \n",
        "    # Initialise paramaters\n",
        "    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    points_covered = 0\n",
        "    max_accuracy = []\n",
        "    \n",
        "    # Loop\n",
        "    for i in range(num_epochs):\n",
        "      # Initialise gradients\n",
        "      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "      arr  = np.arange(54000)\n",
        "      np.random.shuffle(arr)\n",
        "\n",
        "      for j in range(0,54000):\n",
        "          ind = arr[j]\n",
        "          x = x_train[ind,:]\n",
        "          y = y_train[ind,:]\n",
        "          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "         \n",
        "          # weight updates\n",
        "          for key in grad:\n",
        "              grad[key] = grad[key] + grad_t[key]\n",
        "\n",
        "          points_covered = points_covered + 1\n",
        "\n",
        "          if points_covered % batch_size == 0:\n",
        "              for keynew in theta:\n",
        "                momenta[\"m\" + keynew] = gamma*prev_momenta[\"m\"+ keynew] + eta*(grad[\"d\" + keynew])\n",
        "                theta[keynew] = theta[keynew] - momenta[\"m\" + keynew]\n",
        "                prev_momenta[\"m\" + keynew] = momenta[\"m\" + keynew] \n",
        "\n",
        "              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "      \n",
        "      loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "\n",
        "    #return theta\n",
        "\n",
        "# Nesterov Accelarated Gradient Descent (Lecture 5)\n",
        "\n",
        "def nesterov_gradient_descent(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, gamma):\n",
        "                              \n",
        "    # Initialise paramaters\n",
        "    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    points_covered = 0\n",
        "    max_accuracy = []\n",
        "\n",
        "    # Loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        # Initialise gradients\n",
        "        grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "        arr  = np.arange(54000)\n",
        "        np.random.shuffle(arr)\n",
        "\n",
        "        for j in range(0,54000):\n",
        "          ind = arr[j]\n",
        "          x = x_train[ind,:]\n",
        "          y = y_train[ind,:]\n",
        "          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "          # calculating gradients after partial updates \n",
        "          for key in grad:\n",
        "              grad[key] = grad[key] + grad_t[key]\n",
        "          \n",
        "          points_covered = points_covered + 1\n",
        "\n",
        "          if points_covered % batch_size == 0:\n",
        "              # full update\n",
        "              for key in theta:\n",
        "                momenta[\"m\" + key] = gamma*prev_momenta[\"m\"+ key] + eta*(grad[\"d\" + key])\n",
        "                theta[key] = theta[key] - momenta[\"m\" + key]\n",
        "                prev_momenta[\"m\" + key] = momenta[\"m\" + key] \n",
        "              \n",
        "              # initialise gradients after every batch is done\n",
        "              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "              # partial updates\n",
        "              for key in theta:\n",
        "                momenta[\"m\" + key] = gamma*prev_momenta[\"m\" + key]\n",
        "                grad[\"d\" + key] = grad[\"d\" + key] - momenta[\"m\" + key]\n",
        "        \n",
        "\n",
        "        loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "      \n",
        "    #return theta\n",
        "\n",
        "# Stochatic gradient descent (Lecture 5)\n",
        "\n",
        "def stochastic_gradient_descent(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, gamma):\n",
        "                                \n",
        "    # Initialise paramaters\n",
        "    \n",
        "    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "    points_covered = 0\n",
        "    max_accuracy = []\n",
        "\n",
        "    # Loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        # Initialise gradients\n",
        "        grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "        arr  = np.arange(54000)\n",
        "        np.random.shuffle(arr)\n",
        "\n",
        "        for j in range(0,54000):\n",
        "          ind = arr[j]\n",
        "          x = x_train[ind,:]\n",
        "          y = y_train[ind,:]\n",
        "          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "          for key in grad:\n",
        "            grad[key] = grad[key] + grad_t[key]\n",
        "          \n",
        "          points_covered = points_covered + 1\n",
        "\n",
        "          if points_covered % batch_size == 0:\n",
        "            # seen one mini batch\n",
        "            for key in theta:\n",
        "              theta[key] = theta[key] - eta*(grad[\"d\" + key])\n",
        "\n",
        "            # Initialise gradients\n",
        "            grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "        loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "        \n",
        "    #return theta  \n",
        "\n",
        "# The following functions creates the square momenta for to be used in the \n",
        "# update rule in the RMSprop, adam optimiser\n",
        "\n",
        "def init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size):\n",
        "    size = [] # this contains the list of number of nodes of every layer in the network\n",
        "    for j in range(number_hidden):\n",
        "      size.append(hid_input_size)\n",
        "    size = [net_input_size] + size + [net_output_size]\n",
        "    momenta = {}\n",
        "    for i in range(1, number_hidden+2):\n",
        "        momenta[\"vW\" + str(i)] = np.zeros((size[i], size[i-1]))\n",
        "        momenta[\"vb\" + str(i)] = np.zeros((size[i],1))\n",
        "    return momenta\n",
        "\n",
        "\n",
        "# RMSprop (Lecture 5)\n",
        "def RMSprop(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, eps, beta):\n",
        "\n",
        "  # Initialise paramaters\n",
        "\n",
        "  theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "  prev_momenta = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "  momenta = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "  points_covered = 0\n",
        "  max_accuracy = []\n",
        "\n",
        "  # Loop\n",
        "  for i in range(num_epochs):\n",
        "\n",
        "       # Initialise gradients\n",
        "      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "      arr  = np.arange(54000)\n",
        "      np.random.shuffle(arr)\n",
        "\n",
        "      for j in range(0,54000):\n",
        "        ind = arr[j]\n",
        "        x = x_train[ind,:]\n",
        "        y = y_train[ind,:]\n",
        "        h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "        grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "        for key in grad:\n",
        "            grad[key] = grad[key] + grad_t[key]\n",
        "          \n",
        "        points_covered = points_covered + 1\n",
        "\n",
        "        if points_covered % batch_size == 0:\n",
        "\n",
        "          # Update rule for RMSprop\n",
        "          for keynew in theta:\n",
        "            momenta[\"v\" + keynew] = beta*prev_momenta[\"v\"+ keynew] + (1 - beta)*((grad[\"d\" + keynew])**2)\n",
        "            theta[keynew] = theta[keynew] - (eta/ (np.sqrt(momenta[\"v\" + keynew] + eps)) )*(grad[\"d\" + keynew]) \n",
        "            prev_momenta[\"v\" + keynew] = momenta[\"v\" + keynew] \n",
        "          \n",
        "          # Initialise gradients\n",
        "          grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "      loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "\n",
        "  #return theta   \n",
        "\n",
        "# Adam optimiser (Lecture 5)\n",
        "\n",
        "def adam(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, eps, beta1, beta2 ):\n",
        "\n",
        "    # Initialise paramaters\n",
        "\n",
        "    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "\n",
        "    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_bias = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "    prev_momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_sq_bias = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "    points_covered = 0\n",
        "    max_accuracy = []\n",
        "\n",
        "    # Loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "      # Initialise gradients\n",
        "      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "      time_count = 0\n",
        "      arr  = np.arange(54000)\n",
        "      np.random.shuffle(arr)\n",
        "\n",
        "      for j in range(0,54000):\n",
        "          ind = arr[j]\n",
        "          x = x_train[ind,:]\n",
        "          y = y_train[ind,:]\n",
        "          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "          for key in grad:\n",
        "              grad[key] = grad[key] + grad_t[key]\n",
        "            \n",
        "          points_covered = points_covered + 1\n",
        "\n",
        "          if points_covered % batch_size == 0:\n",
        "              time_count = time_count + 1\n",
        "\n",
        "              for key in theta:\n",
        "\n",
        "                  # First moments\n",
        "                  momenta[\"m\" + key] = beta1*prev_momenta[\"m\" + key] + (1 - beta1) * grad[\"d\" + key]\n",
        "                  momenta_bias[\"m\" + key] = momenta[\"m\" + key]/(1 - np.power(beta1, time_count)) #bias correction\n",
        "\n",
        "                  # Second moments\n",
        "                  momenta_sq[\"v\" + key] = beta2*prev_momenta_sq[\"v\" + key] + (1 - beta2)*((grad[\"d\" + key])**2)\n",
        "                  momenta_sq_bias[\"v\" + key] = momenta_sq[\"v\" + key]/(1 - np.power(beta2, time_count)) #bias correction\n",
        "\n",
        "                  #w_t+1\n",
        "                  theta[key] = theta[key] - (eta/np.sqrt(momenta_sq_bias[\"v\" + key] + eps))*momenta_bias[\"m\" + key]\n",
        "\n",
        "                  prev_momenta[\"m\" + key] = momenta[\"m\" + key]\n",
        "                  prev_momenta_sq[\"v\" + key] = momenta_sq[\"v\" + key]\n",
        "              \n",
        "              # Initialise gradients\n",
        "              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "      loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "  \n",
        "    #return theta\n",
        "\n",
        "# Nadam optimiser (NAG + Adam)\n",
        "\n",
        "def nadam(num_epochs, number_hidden, hid_input_size, regpara, eta, batch_size, act_func, loss_func, weight_init, net_input_size, net_output_size, eps, beta1, beta2 ):\n",
        "\n",
        "    # Initialise paramaters\n",
        "\n",
        "    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )\n",
        "\n",
        "    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_bias = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "    prev_momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "    momenta_sq_bias = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "    points_covered = 0\n",
        "    max_accuracy = []\n",
        "\n",
        "    # Loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "      # Initialise gradients\n",
        "      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "      time_count = 0\n",
        "      arr  = np.arange(54000)\n",
        "      np.random.shuffle(arr)\n",
        "\n",
        "      for j in range(0,54000):\n",
        "          ind = arr[j]\n",
        "          x = x_train[ind,:]\n",
        "          y = y_train[ind,:]\n",
        "          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   \n",
        "          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func, regpara)\n",
        "\n",
        "          for key in grad:\n",
        "              grad[key] = grad[key] + grad_t[key]\n",
        "            \n",
        "          points_covered = points_covered + 1\n",
        "\n",
        "          if points_covered % batch_size == 0:\n",
        "              time_count = time_count + 1\n",
        "\n",
        "              for key in theta:\n",
        "\n",
        "                  # First moments\n",
        "                  momenta[\"m\" + key] = beta1*prev_momenta[\"m\" + key] + (1 - beta1) * grad[\"d\" + key]\n",
        "                  adam_bias = momenta[\"m\" + key]/(1 - np.power(beta1, time_count)) \n",
        "                  momenta_bias[\"m\" + key] = beta1*adam_bias + (1 - beta1) * grad[\"d\" + key] #bias correction\n",
        "\n",
        "                  # Second moments\n",
        "                  momenta_sq[\"v\" + key] = beta2*prev_momenta_sq[\"v\" + key] + (1 - beta2)*((grad[\"d\" + key])**2)\n",
        "                  momenta_sq_bias[\"v\" + key] = momenta_sq[\"v\" + key]/(1 - np.power(beta2, time_count)) #bias correction\n",
        "\n",
        "                  #w_t+1\n",
        "                  theta[key] = theta[key] - (eta/np.sqrt(momenta_sq_bias[\"v\" + key] + eps))*momenta_bias[\"m\" + key]\n",
        "\n",
        "                  prev_momenta[\"m\" + key] = momenta[\"m\" + key]\n",
        "                  prev_momenta_sq[\"v\" + key] = momenta_sq[\"v\" + key]\n",
        "              \n",
        "              # Initialise gradients\n",
        "              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)\n",
        "\n",
        "      #val_acc, val_loss, train_acc, train_loss = loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "      loss_accuracy_compute(i, num_epochs, max_accuracy, x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, regpara)\n",
        "  \n",
        "    #return theta, val_acc, val_loss, train_acc, train_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrrFYGHHZAYn"
      },
      "source": [
        "# Configure the sweep \n",
        "# Specify the method, metric, parameters to search through\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'early_terminate': {\n",
        "            'type': 'hyperband',\n",
        "            'min_iter': [3],\n",
        "            's': [2]\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5, 10] #number of epochs\n",
        "        },\n",
        "        'number_hidden': {\n",
        "            'values': [3, 4, 5] #number of hidden layers\n",
        "        },\n",
        "        'hidden_inputsize': {\n",
        "            'values': [32, 64, 128] #size of every hidden layer\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005,  0.5] #L2 regularisation\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4] \n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': [ 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size' : {\n",
        "            'values':[16, 32, 64]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values': ['random','xavier']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['sigmoid','tanh','relu']\n",
        "        }\n",
        "        \n",
        "        }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6QUWA-Ubz2n"
      },
      "source": [
        "# Initialize a new sweep\n",
        "# Arguments:\n",
        "# param_config: the sweep config dictionary defined above\n",
        "# entity: Set the username for the sweep\n",
        "# project: Set the project name for the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"bharatik\", project=\"cs6910assignment1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wR5k3DDdTxx"
      },
      "source": [
        "# The sweep calls this function with each set of hyperparameters\n",
        "\n",
        "def train():\n",
        "    # Default values for hyper-parameters that we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 5,\n",
        "        'number_hidden': 3,\n",
        "        'hidden_inputsize': 32,\n",
        "        'weight_decay': 0,\n",
        "        'learning_rate': 1e-3,\n",
        "        'optimizer': 'momentum',\n",
        "        'batch_size': 64,\n",
        "        'activation': 'sigmoid',\n",
        "        'weight_init': 'random',\n",
        "        'loss' : 'squared',\n",
        "        'gamma' : 0.9, # update parameter\n",
        "        'net_input_size' : 784, \n",
        "        'net_output_size' : 10,\n",
        "        'eps' : 1e-8,\n",
        "        'beta': 0.95,\n",
        "        'beta1' : 0.9,\n",
        "        'beta2' : 0.999\n",
        "    }\n",
        "\n",
        "     # Initializing a new wandb run\n",
        "    #wandb.init(config=config_defaults,resume=True)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config \n",
        "    wandb.run.name = \"hl_\" + str(config.hidden_inputsize)+\"_bs_\"+str(config.batch_size)+\"_ac_\"+ config.activation + \"_loss_\" + config.loss\n",
        "\n",
        "    # Defining the various optimizers\n",
        "\n",
        "    if config.optimizer=='adam':\n",
        "      adam(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, eps = config.eps, beta1=config.beta1, beta2 = config.beta2)\n",
        "    elif config.optimizer=='nadam':\n",
        "      nadam(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, eps = config.eps, beta1=config.beta1, beta2 = config.beta2)\n",
        "    elif config.optimizer=='sgd':\n",
        "      stochastic_gradient_descent(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, gamma = config.gamma)\n",
        "    elif config.optimizer=='rmsprop':\n",
        "      RMSprop(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, eps = config.eps, beta=config.beta)\n",
        "    elif config.optimizer=='momentum':\n",
        "      momentum_based_gradient_descent(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, gamma = config.gamma)\n",
        "    elif config.optimizer=='nesterov':\n",
        "      nesterov_gradient_descent(num_epochs = config.epochs, number_hidden = config.number_hidden, hid_input_size = config.hidden_inputsize, regpara = config.weight_decay, eta = config.learning_rate, batch_size = config.batch_size, act_func = config.activation, loss_func = config.loss, weight_init = config.weight_init, net_input_size = config.net_input_size, net_output_size = config.net_output_size, gamma = config.gamma)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txVRzcuqVt-J"
      },
      "source": [
        "wandb.agent('91idz382',train,count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJgETLeKNfl"
      },
      "source": [
        "#Best hyperparameters for validation accuracy of 0.88717\n",
        "\n",
        "best_num_epochs = 10\n",
        "best_number_hidden = 3\n",
        "best_hid_input_size = 128\n",
        "best_regpara = 0.0005\n",
        "best_eta = 0.001\n",
        "best_batch_size = 16\n",
        "best_act_func = 'relu'\n",
        "best_loss_func = 'cross'\n",
        "best_weight_init = 'xavier'\n",
        "\n",
        "theta  = nadam(num_epochs = best_num_epochs, number_hidden = best_number_hidden, hid_input_size = best_hid_input_size, regpara = best_regpara, eta = best_eta, batch_size = best_batch_size, act_func = best_act_func, loss_func = best_loss_func, weight_init = best_weight_init, net_input_size = 784, net_output_size = 10, eps = 1e-8, beta1 = 0.9, beta2=0.999)\n",
        "\n",
        "def test_loss_accuracy_compute(best_num_epochs, x_test, y_test, theta, best_number_hidden, best_act_func, best_loss_func, best_regpara):\n",
        "\n",
        "    sum_norm = 0\n",
        "    for i in range(1,best_number_hidden + 2):\n",
        "        sum_norm = sum_norm + np.sum(np.square(theta[\"W\"+str(i)]))\n",
        "\n",
        "    test_loss = 0 # training loss\n",
        "    ctr = 0\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    for j in range(0,10000):\n",
        "        x = x_test[j,:]\n",
        "        y = y_test[j,:]\n",
        "        h,a,y_hat = forward_propogation(x, best_act_func, theta, best_number_hidden)\n",
        "        \n",
        "      \n",
        "        if best_loss_func == \"cross\":\n",
        "            test_loss = test_loss + (cross_entropy_loss(y_hat,y,sum_norm, best_regpara))\n",
        "        else:\n",
        "            test_loss = test_loss + (squared_loss(y, y_hat,sum_norm, best_regpara))\n",
        "           \n",
        "        # convert one hot encoded vector to label \n",
        "        y = np.argmax(y, axis = 0)\n",
        "        y_hat = np.argmax(y_hat, axis = 0)\n",
        "        true_label.append(y)\n",
        "        pred_label.append(y_hat)\n",
        "\n",
        "        if y == y_hat:\n",
        "          ctr = ctr + 1\n",
        "\n",
        "\n",
        "    test_acc = ctr/10000.0\n",
        "    test_loss = test_loss/10000.0\n",
        "\n",
        "    return true_label, pred_label, test_acc, test_loss\n",
        "\n",
        "\n",
        "true_label, pred_label, test_acc, test_loss = test_loss_accuracy_compute(best_num_epochs, x_test, y_test, theta, best_number_hidden, best_act_func, best_loss_func, best_regpara)\n",
        "ctr = 0\n",
        "\n",
        "for i in range(10000): \n",
        "  if true_label[i] == pred_label[i].tolist():\n",
        "    ctr = ctr + 1\n",
        "\n",
        "test_accuracy = ctr/10000.0\n",
        "\n",
        "#Plotting confusion matrix\n",
        "\n",
        "cm = confusion_matrix(true_label, pred_label)\n",
        "\n",
        "#wandb.init(project=\"cs6910assignment1\", entity=\"bharatik\")\n",
        "img  = pl.matshow(cm)\n",
        "pl.title('Confusion matrix of the classifier')\n",
        "pl.colorbar()\n",
        "pl.show()\n",
        "\n",
        "#wandb.log({\"confusion matrix\": [ wandb.Image(img, caption='confusion matrix') ]})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LCenf3h1CJC"
      },
      "source": [
        "#MNIST handwritten\n",
        "\n",
        "#wandb.init(project=\"cs6910assignment1\", entity=\"bharatik\")\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Split data for cross validation\n",
        "x_val = x_train[54000:]\n",
        "y_val = y_train[54000:]\n",
        "x_train = x_train[:54000]\n",
        "y_train = y_train[:54000]  \n",
        "\n",
        "image = [];\n",
        "label = [];\n",
        "for i in range(54000):\n",
        "  if len(label) >= 10:\n",
        "    break;\n",
        "  if class_names[y_train[i]] not in label:\n",
        "      image.append(x_train[i])\n",
        "      label.append(class_names[y_train[i]])\n",
        "#wandb.log({\"Examples\": [ wandb.Image(img, caption=caption) for img, caption in zip(image,label)]})\n",
        "\n",
        "# Vectorise and normalize the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_val  = x_val.reshape(x_val.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_val  = x_val / 255.0\n",
        "\n",
        "# One hot encoding for labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_val   = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "#theta, val_acc, val_loss, train_acc, train_loss = nadam(num_epochs = best_num_epochs, number_hidden = best_number_hidden, hid_input_size = best_hid_input_size, regpara = best_regpara, eta = best_eta, batch_size = best_batch_size, act_func = best_act_func, loss_func = best_loss_func, weight_init = best_weight_init, net_input_size = 784, net_output_size = 10, eps = 1e-8, beta1 = 0.9, beta2=0.999)\n",
        "#true_label, pred_label, test_acc, test_loss = test_loss_accuracy_compute(best_num_epochs, x_test, y_test, theta, best_number_hidden, best_act_func, best_loss_func, best_regpara)\n",
        "ctr = 0\n",
        "\n",
        "for i in range(10000): \n",
        "  if true_label[i] == pred_label[i].tolist():\n",
        "    ctr = ctr + 1\n",
        "\n",
        "test_accuracy = ctr/10000.0\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
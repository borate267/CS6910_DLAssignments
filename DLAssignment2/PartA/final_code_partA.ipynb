{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "final_code_partA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrXSX4MjxsY2"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aATW_9CRxiIg"
      },
      "source": [
        "This section contains implementation specifics of building a CNN based image classifier using the iNaturalist dataset.\n",
        "\n",
        "The Architecture:\n",
        "1.   Five convolution layers with each layer followed by a \n",
        "ReLU activation and a max pooling layer.\n",
        "2.   One dense layer \n",
        "3.   One output layer containing 10 neurons (1 for each of the 10 classes). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYiM_Whbx0pu"
      },
      "source": [
        "Import essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "DMMDN6JraJ5S"
      },
      "source": [
        "# Essentials\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Activation\n",
        "\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0m3yWmVgaJ5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673aaf90-09e0-4d62-ef3c-47cfea4fc9bf"
      },
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 19.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 16.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2IlLNHxv5u"
      },
      "source": [
        "Fetch dataset from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prsXWfw8b9Pr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46774d5-594a-4e56-eebd-fd887bc68b59"
      },
      "source": [
        "# Fetch the dataset from Github\n",
        "!git clone https://github.com/borate267/inaturalist-dataset.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'inaturalist-dataset'...\n",
            "remote: Enumerating objects: 12027, done.\u001b[K\n",
            "remote: Total 12027 (delta 0), reused 0 (delta 0), pack-reused 12027\u001b[K\n",
            "Receiving objects: 100% (12027/12027), 3.55 GiB | 50.24 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Checking out files: 100% (11999/11999), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ-6pgCvx5Kv"
      },
      "source": [
        "Read the training and validation images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LphhtX-0aJ5d"
      },
      "source": [
        "# Define the labels for the Simpsons characters we're detecting\n",
        "class_names = {0:'Amphibia', 1:'Animalia', 2:'Arachnida',3: 'Aves',4: 'Fungi',\n",
        "              5: 'Insecta', 6:'Mammalia', 7:'Mollusca', 8:'Plantae',9: 'Reptilia'}\n",
        "num_classes = 10\n",
        "img_size = 128\n",
        "dir = 'inaturalist-dataset/train'\n",
        "\n",
        "import random\n",
        "\n",
        "# Load training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "for label, name in class_names.items():\n",
        "   list_images = os.listdir(dir+'/'+name)\n",
        "   for image_name in list_images:\n",
        "       image = imageio.imread(dir+'/'+name+'/'+image_name)\n",
        "       if np.ndim(image) == 3:\n",
        "          X_train.append(cv2.resize(image, (img_size,img_size)))\n",
        "          y_train.append(label)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzIqdJNLx77-"
      },
      "source": [
        "Shuffle the images and then retain 10% as validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE0f5kewFTpG"
      },
      "source": [
        "leng = np.shape(X_train)\n",
        "arr = np.arange(leng[0])\n",
        "np.random.shuffle(arr)\n",
        "X_train_shuf = []\n",
        "y_train_shuf = []\n",
        "X_val_shuf = []\n",
        "y_val_shuf = []\n",
        "\n",
        "for i in range(leng[0]):\n",
        "  if i <= 9000:\n",
        "    X_train_shuf.append(X_train[arr[i]])\n",
        "    y_train_shuf.append(y_train[arr[i]])\n",
        "  else:\n",
        "    X_val_shuf.append(X_train[arr[i]])\n",
        "    y_val_shuf.append(y_train[arr[i]])\n",
        "\n",
        "X_train = np.array(X_train_shuf)\n",
        "y_train = np.array(y_train_shuf)\n",
        "X_val = np.array(X_val_shuf)\n",
        "y_val = np.array(y_val_shuf)\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train/255.0\n",
        "X_val = X_val/255.0\n",
        "\n",
        "# One hot encode the labels \n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_val = np_utils.to_categorical(y_val, num_classes)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMVb9Sj6yD83"
      },
      "source": [
        "Configure the sweep hyperparameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdaHO-3M8ly3"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', \n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'kernel_size':{\n",
        "            'values': [[(3,3),(3,3),(3,3),(3,3),(3,3)], [(3,3),(5,5),(5,5),(7,7),(7,7)], [(7,7),(7,7),(5,5),(5,5),(3,3)], [(3,3),(5,5),(7,7),(9,9),(11,11)] ]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.005]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'elu', 'selu']\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': ['true','false']\n",
        "        },\n",
        "        'filt_org':{\n",
        "            'values': [[32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n",
        "        },\n",
        "        'data_augment': {\n",
        "            'values': ['true','false']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64]\n",
        "        },\n",
        "        'num_dense':{\n",
        "            'values': [64, 128, 256, 512]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll_zFhpaU6cu"
      },
      "source": [
        " Initialize the Sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdc7RBBaU0F3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fcd9e94f-889f-49a3-c216-ae441f9a7873"
      },
      "source": [
        "# Initialize a new sweep\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"bharatik\", project=\"cs6910assignment2\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 72r3w5eh\n",
            "Sweep URL: https://wandb.ai/bharatik/cs6910assignment2/sweeps/72r3w5eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aIhxl7glaJ5k"
      },
      "source": [
        "def train():\n",
        "    \n",
        "    config_defaults = {\n",
        "        'kernel_size': [(3,3),(3,3),(3,3),(3,3),(3,3)],\n",
        "        'weight_decay': 0.005,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 1e-3,\n",
        "        'activation': 'relu',\n",
        "        'batch_size': 64,\n",
        "        'epochs': 10,\n",
        "        'batch_norm': 'true',\n",
        "        'filt_org' : [32,32,32,32,32],\n",
        "        'conv_layer_size' : 16,\n",
        "        'data_augment': 'true',\n",
        "        'num_dense': 256,\n",
        "        'seed': 42,\n",
        "        'num_classes': 10\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'num_dense_'+ str(config.num_dense)+'_bs_'+str(config.batch_size)+'_ac_'+ config.activation\n",
        "    \n",
        "    # Determine input shape\n",
        "    input_shape = (img_size, img_size , 3)\n",
        "    \n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "\n",
        "    filter = config.filt_org\n",
        "\n",
        "    # Layer one\n",
        "    model.add(Conv2D(filters = filter[0], kernel_size = config.kernel_size[0],padding = 'same', \n",
        "                    input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "\n",
        "    if config.activation == \"relu\":\n",
        "        model.add(Activation('relu'))\n",
        "    elif config.activation == \"elu\":\n",
        "        model.add(Activation('elu'))\n",
        "    elif config.activation == \"selu\":\n",
        "        model.add(Activation('selu'))\n",
        "\n",
        "    if config.batch_norm == 'True':\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "    # Layer two\n",
        "    model.add(Conv2D(filters = filter[1], kernel_size = config.kernel_size[1], padding = 'same', \n",
        "                    input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "\n",
        "    if config.activation == \"relu\":\n",
        "        model.add(Activation('relu'))\n",
        "    elif config.activation == \"elu\":\n",
        "        model.add(Activation('elu'))\n",
        "    elif config.activation == \"selu\":\n",
        "        model.add(Activation('selu'))\n",
        "\n",
        "    if config.batch_norm == 'True':\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "    # Layer three\n",
        "    model.add(Conv2D(filters = filter[2], kernel_size = config.kernel_size[2], padding = 'same', \n",
        "                    input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "\n",
        "    if config.activation == \"relu\":\n",
        "        model.add(Activation('relu'))\n",
        "    elif config.activation == \"elu\":\n",
        "        model.add(Activation('elu'))\n",
        "    elif config.activation == \"selu\":\n",
        "        model.add(Activation('selu'))\n",
        "\n",
        "    if config.batch_norm == 'True':\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer four\n",
        "    model.add(Conv2D(filters = filter[3], kernel_size = config.kernel_size[3], padding = 'same', \n",
        "                    input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "\n",
        "    if config.activation == \"relu\":\n",
        "        model.add(Activation('relu'))\n",
        "    elif config.activation == \"elu\":\n",
        "        model.add(Activation('elu'))\n",
        "    elif config.activation == \"selu\":\n",
        "        model.add(Activation('selu'))\n",
        "\n",
        "    if config.batch_norm == 'True':\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "\n",
        "    # Layer five\n",
        "    model.add(Conv2D(filters = filter[4], kernel_size = config.kernel_size[4], padding = 'same', \n",
        "                    input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "\n",
        "    if config.activation == \"relu\":\n",
        "        model.add(Activation('relu'))\n",
        "    elif config.activation == \"elu\":\n",
        "        model.add(Activation('elu'))\n",
        "    elif config.activation == \"selu\":\n",
        "        model.add(Activation('selu'))\n",
        "\n",
        "    if config.batch_norm == 'True':\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # FC layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(config.num_dense, activation = config.activation, kernel_regularizer = regularizers.l2(config.weight_decay)))\n",
        "    model.add(Dropout(config.dropout))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(num_classes, activation = \"softmax\"))\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = Adam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "\n",
        "    #data augmentation\n",
        "    if config.data_augment == 'true':\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False  # randomly flip images\n",
        "        )\n",
        "    else:\n",
        "        datagen = ImageDataGenerator(rescale = 1.0)\n",
        "\n",
        "    datagen.fit(X_train)\n",
        "    \n",
        "    model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size = config.batch_size),\n",
        "        epochs = config.epochs,\n",
        "        verbose = 1,\n",
        "        validation_data= (X_val, y_val),\n",
        "        callbacks = [WandbCallback()]\n",
        "    )\n",
        "    \n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVxXIXXTyOLC"
      },
      "source": [
        "Run the sweep agent for 100 runs or more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gD9qhA9yOYs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "90291d42-d16e-4738-d6ec-0b34f6824c86"
      },
      "source": [
        "# Initialize a new sweep\n",
        "\n",
        "wandb.agent(sweep_id, train, count = 20)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e7pfmxe9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilt_org: [128, 128, 64, 64, 32]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dense: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbharatik\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.26<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">olive-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/bharatik/cs6910assignment2\" target=\"_blank\">https://wandb.ai/bharatik/cs6910assignment2</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/bharatik/cs6910assignment2/sweeps/72r3w5eh\" target=\"_blank\">https://wandb.ai/bharatik/cs6910assignment2/sweeps/72r3w5eh</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/bharatik/cs6910assignment2/runs/e7pfmxe9\" target=\"_blank\">https://wandb.ai/bharatik/cs6910assignment2/runs/e7pfmxe9</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210415_154433-e7pfmxe9</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 58/141 [===========>..................] - ETA: 22s - loss: 2.6902 - accuracy: 0.1114"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}

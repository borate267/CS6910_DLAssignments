{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9Ss19YSvKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9547dc6-65db-4955-a6f9-8a534f9fe9df"
      },
      "source": [
        "!pip install wandb -qqq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 28.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 18.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkrKGdxAI62i"
      },
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from scipy.special import softmax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzeYoDKX-J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ab3fb6-9e05-4da6-aebc-ea1379bf1619"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbharatik\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVTZZeCXf-Wv"
      },
      "source": [
        "#wandb.init(project=\"cs6910assignment1\", entity=\"bharatik\")\n",
        "\n",
        "# Question 1: Logging images on wandb\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "def loadData():\n",
        "  # Load the data (60000 training and 10000 test samples)\n",
        "  # 10% of training data is for validation \n",
        "\n",
        "  fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_val = x_train[55000:]\n",
        "  y_val = y_train[55000:]\n",
        "  x_train = x_train[:55000]\n",
        "  y_train = y_train[:55000] \n",
        " \n",
        "  return {\n",
        "      'x_train': x_train,\n",
        "      'y_train': y_train, \n",
        "      'x_val'  : x_val,\n",
        "      'y_val'  : y_val,\n",
        "      'x_test' : x_test,\n",
        "      'y_test' : y_test,\n",
        "  }\n",
        "\n",
        "def display_images():\n",
        "  dataset = loadData()\n",
        "  training_images = dataset['x_train']\n",
        "  training_labels = dataset['y_train']\n",
        "  image = [];\n",
        "  label = [];\n",
        "  for i in range(55000):\n",
        "    if len(label) >= 10:\n",
        "      break;\n",
        "    if class_names[training_labels[i]] not in label:\n",
        "        image.append(training_images[i])\n",
        "        label.append(class_names[training_labels[i]])\n",
        "  wandb.log({\"Examples\": [ wandb.Image(img, caption=caption) for img, caption in zip(image,label)]})\n",
        "\n",
        "display_images();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idVAKwoo8TF9"
      },
      "source": [
        "# Q2: Implementing a feed forward neural network\n",
        "\n",
        "# Parameters\n",
        "\n",
        "number_hidden = 2\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "max_iterations = 1000\n",
        "eta = 0.01\n",
        "\n",
        "# Preprocessing the data\n",
        "\n",
        "def preprocess():\n",
        "  dataset = loadData();\n",
        "  training_images = dataset['x_train']\n",
        "  training_labels = dataset['y_train']\n",
        "  testing_images = dataset['x_test']\n",
        "  testing_labels = dataset['y_test']\n",
        "\n",
        "  # Vectorising the images and preprocessing\n",
        "  train_data = training_images.reshape(training_images.shape[0], input_size)\n",
        "  test_data = testing_images.reshape(testing_images.shape[0], input_size)\n",
        "  train_data = train_data/255.0\n",
        "  test_data  = test_data/255.0\n",
        "\n",
        "  # One hot encoding for labels\n",
        "  Enctrain_label = to_categorical(training_labels)\n",
        "  Enctest_label = to_categorical(testing_labels)\n",
        "  return train_data, test_data, Enctrain_label, Enctest_label\n",
        "\n",
        "train_data, test_data, Enctrain_label, Enctest_label = preprocess();\n",
        "\n",
        "# Creating a network of neurons\n",
        "\n",
        "def create_network(number_hidden, input_size, output_size):\n",
        "  #Initialising the parameters\n",
        "  theta0 = {}\n",
        "  for i in range(1, number_hidden+2):\n",
        "    if i <= number_hidden:\n",
        "      theta0[\"W\" + str(i)] = np.random.randn(input_size,input_size)\n",
        "      theta0[\"b\" + str(i)] = np.random.randn(input_size,1)\n",
        "    else:\n",
        "      theta0[\"W\" + str(i)] = np.random.randn(input_size,output_size)\n",
        "      theta0[\"b\" + str(i)] = np.random.randn(output_size,1)\n",
        "  return theta0\n",
        "\n",
        "# Forward Propogation\n",
        "\n",
        "def forward_propogation(x, theta, number_hidden, input_size):\n",
        "  a = {} #pre-activation values\n",
        "  h = {} #activation values\n",
        "  h = {\"h0\": x}\n",
        "\n",
        "  for i in range(1, number_hidden + 2):\n",
        "    if i != number_hidden + 1:\n",
        "      W_current = theta0[\"W\" + str(i)]\n",
        "      b_current = theta0[\"b\" + str(i)]\n",
        "      h_previous = h[\"h\" + str(i-1)]\n",
        "      a_current = np.dot(W_current,h_previous) + b_current\n",
        "      a[\"a\" + str(i)] = a_current\n",
        "      h_current = sigmoid_activation(a_current)\n",
        "      h[\"h\" + str(i)] = h_current\n",
        "\n",
        "    else:\n",
        "      a_current = np.dot(W_current,h_previous) + b_current\n",
        "      h_current = softmax(a_current)  \n",
        "      h[\"h\" + str(i)] = h_current\n",
        "      \n",
        "  y_hat = h[\"h\" + str(number_hidden + 1)]\n",
        "  return h, a, y_hat\n",
        "\n",
        "\n",
        "def backward_propogation(h, a, y_hat, Enctrain_label,number_hidden, input_size, output_size):\n",
        "  return 0;\n",
        "\n",
        "# This function is used for activation at the hidden layers\n",
        "\n",
        "def sigmoid_activation(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# Returns gradient of sigmoid function\n",
        "\n",
        "def grad_sigmoid(z):\n",
        "    return (sigmoid_activation(z))*(1 - sigmoid_activation(z))\n",
        "\n",
        "# This function is used for activation at the output layer\n",
        "\n",
        "def softmax_activation(x):\n",
        "  return softmax(x)\n",
        "\n",
        "# This function calculates the cross entropy loss \n",
        "\n",
        "def cross_entropy_loss(p,q):\n",
        "  p = np.array(p).reshape(-1)\n",
        "  q = np.array(q).reshape(-1)\n",
        "  logp = np.log(p)\n",
        "  loss_vec = (-1)*q*logp\n",
        "  loss = np.sum(loss_vec)\n",
        "  return loss    \n",
        "\n",
        "# Create the gradients\n",
        "\n",
        "def creategrads( num_hidden, sizes, inputsize, outputsize ):\n",
        "    sizes = [inputsize] + sizes\n",
        "    sizes = sizes + [outputsize]\n",
        "    grads = {\"dh0\":np.zeros((inputsize,1)),\n",
        "            \"da0\":np.zeros((inputsize,1))}\n",
        "    for i in range(1, num_hidden+2):\n",
        "        grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "        grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "        grads[\"da\" + str(i)] = np.zeros((sizes[i],1))\n",
        "        grads[\"dh\" + str(i)] = np.zeros((sizes[i],1))\n",
        "        \n",
        "\n",
        "    return grads\n",
        "\n",
        "# Gradient Descent\n",
        "\n",
        "def gradient_descent(max_iterations, theta, train_data, Enctrain_label, eta, number_hidden, input_size):\n",
        "  t = 0;\n",
        "  datapointCount = 0;\n",
        "  theta0 = create_network(number_hidden, input_size, output_size) # Initialise paramaters\n",
        "  \n",
        "  for i in range(max_iterations):\n",
        "\n",
        "    # Initialise gradients\n",
        "\n",
        "    for x,y in zip(train_data, Enctrain_label):\n",
        "        x = train_data[j,:]\n",
        "        y = Enctest_label[j,:]\n",
        "        h, a, y_hat = forward_propogation(x, theta, number_hidden, input_size)\n",
        "        grad_theta = backward_propogation(h, a, y_hat, Enctrain_label, number_hidden, input_size, output_size)\n",
        "\n",
        "    theta = theta - eta*grad_theta;\n",
        "    t = t + 1;\n",
        "  return theta\n",
        "\n",
        "\n",
        "weights = gradient_descent(max_iterations, theta0, train_data, Enctrain_label, eta, number_hidden, input_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaUPvfLbUGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7c2004-48d9-4db8-c214-af1d7930a827"
      },
      "source": [
        "np.arange(1,3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVx7FV02XIau"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
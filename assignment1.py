# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MYRTmuadLWa8EHhKJpJ65D3njzRxR_n5
"""

# Commented out IPython magic to ensure Python compatibility.
""" The following code implements gradient descent and its variants with backpropogation for an image classification problem

Created by Bharati. K EE20D700
"""

# WandB â€“ Install the W&B library
# %pip install wandb -q
import wandb
from wandb.keras import WandbCallback

# Essentials
#import wandb
import numpy as np
import tensorflow as tf
from keras.utils import to_categorical

#Fetch the dataset and visualise the images
from keras.datasets import fashion_mnist

#Define the labels for the images that are being detected
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Load training data
def loadData():
  # Load the data (60000 training and 10000 test samples)
  # 10% of training data is for validation

  fashion_mnist = tf.keras.datasets.fashion_mnist
  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

  # Split data for cross validation  
  x_val = x_train[54000:]
  y_val = y_train[54000:]
  x_train = x_train[:54000]
  y_train = y_train[:54000]  

  return {
      'x_train': x_train,
      'y_train': y_train, 
      'x_val'  : x_val,
      'y_val'  : y_val,
      'x_test' : x_test,
      'y_test' : y_test,
  }


def display_images():
  dataset = loadData()
  training_images = dataset['x_train']
  training_labels = dataset['y_train']
  image = [];
  label = [];
  for i in range(54000):
    if len(label) >= 10:
      break;
    if class_names[training_labels[i]] not in label:
        image.append(training_images[i])
        label.append(class_names[training_labels[i]])
  #wandb.log({"Examples": [ wandb.Image(img, caption=caption) for img, caption in zip(image,label)]})

#wandb.init(project="cs6910assignment1", entity="bharatik")
#display_images();

# Q2: Implementing backpropogation with Vanilla gradient descent (Lecture 4)

# Parameters
number_hidden = 3
net_input_size = 784
net_output_size = 10
num_epochs = 5
eta = 0.001
hid_input_size = 64
act_func = 'relu' 
loss_func = 'cross' 
weight_init = 'random'

# Preprocessing the data
def preprocess():
  dataset = loadData();
  training_images = dataset['x_train']
  training_labels = dataset['y_train']
  testing_images = dataset['x_test']
  testing_labels = dataset['y_test']
  valid_images = dataset['x_val']
  valid_labels = dataset['y_val']

  # Vectorising the images and preprocessing
  train_data = training_images.reshape(training_images.shape[0], net_input_size)
  test_data = testing_images.reshape(testing_images.shape[0], net_input_size)
  val_train_data = valid_images.reshape(valid_images.shape[0], net_input_size)

  train_data = train_data/255.0
  test_data  = test_data/255.0
  val_data   = val_train_data/255.0

  # One hot encoding for labels
  Enctrain_label = to_categorical(training_labels)
  Enctest_label = to_categorical(testing_labels)
  Encval_label = to_categorical(valid_labels)

  return train_data, Enctrain_label, val_data,  Encval_label , test_data, Enctest_label


# The following function initialises the weights and biases for a neural network
# that has (number_hidden + 1) layers.
def init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size ):
    size = [] # this contains the list of number of nodes of every layer in the network
    for j in range(number_hidden):
      size.append(hid_input_size)
    size = [net_input_size] + size + [net_output_size]
    #print(size)
    theta0 = {}

    # Reference: (1) https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/
    #            (2) https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78

    if weight_init == 'xavier':
      if act_func != 'relu': #xavier weight initialisation for sigmoid and tanh
        for i in range(1, number_hidden+2):
          lower = -1/ np.sqrt(size[i-1])
          upper = 1/ np.sqrt(size[i-1])
          theta0["W" + str(i)] = lower + np.random.randn(size[i],size[i-1])*(upper- lower)
          theta0["b" + str(i)] = lower + np.random.randn(size[i],1)*(upper - lower)

      else: #He Weight Initialization for reLU activation
      # generate random number and multiply by a standard deviation that is equal to sqrt(2/number of nodes in previous layer)
        for i in range(1, number_hidden+2): 
          theta0["W" + str(i)] = np.random.randn(size[i], size[i-1])*(np.sqrt(2/(size[i-1])))
          theta0["b" + str(i)] = np.random.randn(size[i], 1)*(np.sqrt(2/(size[i-1])))

    else: #random initialisation
       for i in range(1, number_hidden+2):
          if act_func != 'relu':
            theta0["W" + str(i)] = np.random.randn(size[i], size[i-1])
            theta0["b" + str(i)] = np.zeros((size[i],1))
          else:
            theta0["W" + str(i)] = np.random.randn(size[i], size[i-1])*(np.sqrt(2/(size[i]+size[i-1])))
            theta0["b" + str(i)] = np.zeros((size[i],1))

    return theta0

# The following functions are used for activation at the hidden layers
def sigmoid(x):
	  return 1/(1 + np.exp(-x))

def relu(x):
    return (x>0)*(x) + 0.01*((x<0)*x)

def tanh(z):
	  return np.tanh(z)
   
def activation(x, act_func):
    if act_func == "sigmoid":
        return sigmoid(x)
    elif act_func == "tanh":
        return tanh(x)
    elif act_func == "relu":
        return relu(x)

# This function is used for activation at the output layer
def softmax(x):
    x = x - np.max(x)
    return np.exp(x)/np.sum(np.exp(x),axis=0)

# The following functions return the gradients of the various activation functions 
def grad_sigmoid(x):
    return (sigmoid(x))*(1 - sigmoid(x))

def grad_relu(z): # unit step
    return (z>0)*(np.ones(np.shape(z))) + (z<0)*(0.01*np.ones(np.shape(z)))

def grad_tanh(x):
    return (1 - (tanh(x))**2)

# The following functions are used to calculate loss
def squared_loss(x, y):
    loss = 0.5*np.sum((y-x)**2)
    return loss

def cross_entropy_loss(X, Y):
    x = np.array(X).reshape(-1)
    y = np.array(Y).reshape(-1)
    logx = np.log(x)
    loss_vec = (-1)*(y*logx)
    loss = np.sum(loss_vec)
    return loss    


# Forward Propogation
def forward_propogation(x, act_func, theta, number_hidden):
    a = {} # dictionary of pre-activation values
    h = {} # dictionary of activation values
    # converting to a 2-dimensional array
    if x.ndim == 1:
      x = np.expand_dims(x, axis=1)
    h = {"h0": x} 

    for i in range(1, number_hidden + 2):
      
      if i < number_hidden + 1:

        # computing pre-activation values
        W_current = theta["W" + str(i)]
        b_current = theta["b" + str(i)]
        h_previous = h["h" + str(i-1)]
        a_current = np.dot(W_current,h_previous) + b_current
        a["a" + str(i)] = a_current

        # computing activation values
        h_current = activation(a_current, act_func)
        h["h" + str(i)] = h_current  

      else: # for output layer

        a_current = np.dot(theta["W" + str(i)],h["h" + str(i-1)]) + theta["b" + str(i)]
        h_current = softmax(a_current)  
        h["h" + str(i)] = h_current  

    y_hat = h["h" + str(number_hidden + 1)]

    return h, a, y_hat


# Initialising the gradients for backpropagation. The gradients are computed wrt
# output units, hidden layers, weights and biases
def init_grad(number_hidden, hid_input_size, net_input_size, net_output_size ):
    size = [] # this contains the list of number of nodes of every layer in the network
    for j in range(number_hidden):
        size.append(hid_input_size)
    size = [net_input_size] + size + [net_output_size]

    grad = {} 
    
    for i in range(1, number_hidden + 2):
        grad["da" + str(i)] = np.zeros((size[i],1))
        grad["dh" + str(i)] = np.zeros((size[i],1))
        grad["dW" + str(i)] = np.zeros((size[i],size[i-1]))
        grad["db" + str(i)] = np.zeros((size[i],1))

    #grad = {"dh0":np.zeros((net_input_size,1)), "da0":np.zeros((net_input_size,1))} # is used for back prop computation

    return grad

# The following function implements Backpropogation as described in Lecture 4

def backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func):

    grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size ) 
    
    a["a0"] = np.zeros((net_input_size,1)) 
    
    # Computing gradient wrt output layer
    if y.ndim == 1:
      y = np.expand_dims(y, axis=1) # Converting to a 2-dimensional array
      
    if loss_func == "cross":
        grad["da" + str(number_hidden + 1)] = -1*(y - y_hat) #Enctrain label is one hot encoded
    else:
        grad["da" + str(number_hidden + 1)] = -1*(y - y_hat)*y_hat - y_hat*(np.dot((y_hat- y).T, y_hat))
    
    for i in range(number_hidden + 1 ,0,-1): # L to 1

      # Computing gradients wrt parameters
      grad["dW" + str(i)] = np.dot( grad["da" + str(i)], (h["h" + str(i-1)]).T )
      grad["db" + str(i)] = grad["da" + str(i)]

      # Computing gradients wrt layer below
      grad["dh" + str(i-1)] = np.dot( (theta["W" + str(i)]).T, grad["da" + str(i)] )

      # Computing gradients wrt layer below (preactivation)
      if act_func == "sigmoid":
          grad["da" + str(i-1)] = (grad["dh" + str(i-1)])*(grad_sigmoid(a["a" + str(i-1)]))      
      elif act_func == "tanh":
          grad["da" + str(i-1)] = (grad["dh" + str(i-1)])*(grad_tanh(a["a" + str(i-1)]))
      elif act_func == "relu":
          grad["da" + str(i-1)] = (grad["dh" + str(i-1)])*(grad_relu(a["a" + str(i-1)]))

    return grad


# The function computes accuracies and errors for training and validation

def loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size):

    train_error = 0 # training error
    train_loss = 0 # training loss
    val_error = 0 # validation error
    val_loss = 0 # validation loss

    for j in range(0,54000):
        x = x_train[j,:]
        y = y_train[j,:]
        h,a,y_hat = forward_propogation(x, act_func, theta, number_hidden)
      
        if loss_func == "cross":
            train_loss = train_loss + (cross_entropy_loss(y_hat,y))
        else:
            train_loss = train_loss + (squared_loss(y, y_hat))
        
        # convert one hot encoded vector to label 
        y = np.argmax(y, axis = 0)
        y_hat = np.argmax(y_hat, axis = 0)

        train_acc = 100*(np.sum(y == y_hat)/net_output_size)
        train_error = train_error + (100 - train_acc)
  
    for j in range(0,6000):
        x_vals = x_val[j,:]
        y_vals = y_val[j,:]
        h,a,y_hat_val = forward_propogation(x_vals, act_func, theta, number_hidden)
      
        if loss_func == "cross":
            val_loss = val_loss + (cross_entropy_loss(y_hat_val,y_vals))
        else:
            val_loss = val_loss + (squared_loss(y_vals, y_hat_val))
        
        # convert one hot encoded vector to label 
        y_vals = np.argmax(y_vals, axis = 0)
        y_hat_val = np.argmax(y_hat_val, axis = 0)

        val_acc = 100*(np.sum(y_vals == y_hat_val)/net_output_size)
        val_error = val_error + (100 - val_acc)
      
    return train_error, train_loss, val_error, val_loss

def testing_data(x_test, y_test, theta, act_func, number_hidden):
    test_error = 0
    test_loss = 0
    for j in range(0,100):
        x = x_test[j,:]
        y = y_test[j,:]
        h,a,y_hat_test = forward_propogation(x, act_func, theta, number_hidden)

        if loss_func == "cross":
            test_loss = test_loss + (cross_entropy_loss(y_hat_test,y))
        else:
            test_loss = test_loss + (squared_loss(y, y_hat_test))
        
        # convert one hot encoded vector to label 
        y = np.argmax(y, axis = 0)
        y_hat_test = np.argmax(y_hat_test, axis = 0)

        test_acc = 100*(np.sum(y == y_hat_test)/net_output_size)
        test_error = test_error + (100 - test_acc)
    return test_error/10000, test_loss/10000


#(num_epochs , number_hidden , hinput_size , regpara , eta , batch_size , act_func , loss_func , weight_init)
# Gradient Descent Algorithm (Lecture 4)
def gradient_descent(num_epochs , number_hidden , hid_input_size , act_func , loss_func , weight_init, eta):
  # Initialise paramaters
  theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size ) 
  epoch_data = []

  # Loop
  for i in range(num_epochs):

    grads = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

    for j in range(0,54000):
      x = x_train[j,:]
      y = y_train[j,:]
      h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)  
      grad_current = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)

      for key in grads:
        grads[key] = grads[key] + grad_current[key]

    for keynew in theta:
      theta[keynew] = theta[keynew] - eta*(grads["d" + keynew]) 
    
    train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
    epoch_data.append([i, train_error, train_loss, val_error, val_loss])

  return theta, epoch_data

x_train, y_train, x_test, y_test, x_val, y_val = preprocess();
theta, epoch_data = gradient_descent(num_epochs , number_hidden , hid_input_size , act_func , loss_func , weight_init, eta)
test_error, test_loss = testing_data(x_test, y_test, theta, act_func, number_hidden)
#print(test_error)

# Q3: Implement the backpropagation algorithm with support for the following optimisation functions

# (a) Momentum based Gradient Descent

gamma = 0.9 # update parameter

# The following functions creates the momenta for weights and biases to be used for 
# updating in the GD algorithm
def init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size):
    size = [] # this contains the list of number of nodes of every layer in the network
    for j in range(number_hidden):
      size.append(hid_input_size)
    size = [net_input_size] + size + [net_output_size]
    momenta = {}
    for i in range(1, number_hidden+2):
        momenta["mW" + str(i)] = np.zeros((size[i], size[i-1]))
        momenta["mb" + str(i)] = np.zeros((size[i],1))
    return momenta

# The following function implements momentum based gradient descent (Lecture 5)
def momentum_based_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, batch_size, gamma):
    
    # Initialise paramaters
    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )
    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    points_covered = 0
    epoch_data = []

    # Loop
    for i in range(num_epochs):
      # Initialise gradients
      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)
    
      for j in range(0,100):
          x = x_train[j,:]
          y = y_train[j,:]
          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   
          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)
         
          # weight updates
          for key in grad:
              grad[key] = grad[key] + grad_t[key]

          points_covered = points_covered + 1

          if points_covered % batch_size == 0:
              for keynew in theta:
                momenta["m" + keynew] = gamma*prev_momenta["m"+ keynew] + eta*(grad["d" + keynew])
                theta[keynew] = theta[keynew] - momenta["m" + keynew]
                prev_momenta["m" + keynew] = momenta["m" + keynew] 

              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)
      
      train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
      epoch_data.append([i, train_error, train_loss, val_error, val_loss])

    return theta, epoch_data

theta, epoch_data = momentum_based_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, 100, gamma)

# (b) Nesterov Accelarated Gradient Descent (Lecture 5)

def nesterov_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, batch_size, gamma):

    # Initialise paramaters
    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )
    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    points_covered = 0
    epoch_data = []

    # Loop
    for i in range(num_epochs):

        # Initialise gradients
        grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

        for j in range(0,100):
          x = x_train[j,:]
          y = y_train[j,:]
          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   
          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)

          # calculating gradients after partial updates 
          for key in grad:
              grad[key] = grad[key] + grad_t[key]
          
          points_covered = points_covered + 1

          if points_covered % batch_size == 0:
              # full update
              for key in theta:
                momenta["m" + key] = gamma*prev_momenta["m"+ key] + eta*(grad["d" + key])
                theta[key] = theta[key] - momenta["m" + key]
                prev_momenta["m" + key] = momenta["m" + key] 
              
              # initialise gradients after every batch is done
              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

              # partial updates
              for key in theta:
                momenta["m" + key] = gamma*prev_momenta["m" + key]
                grad["d" + key] = grad["d" + key] - momenta["m" + key]
        

        train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
        epoch_data.append([i, train_error, train_loss, val_error, val_loss])
  
    return theta, epoch_data

theta, epoch_data = nesterov_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, 10, gamma)

# (c) Stochatic gradient descent (Lecture 5)

def stochastic_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, batch_size, gamma):
    
    # Initialise paramaters
    
    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )
    points_covered = 0
    epoch_data = []

    # Loop
    for i in range(num_epochs):

        # Initialise gradients
        grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

        for j in range(0,100):
          x = x_train[j,:]
          y = y_train[j,:]
          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   
          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)

          for key in grad:
            grad[key] = grad[key] + grad_t[key]
          
          points_covered = points_covered + 1

          if points_covered % batch_size == 0:
            # seen one mini batch
            for key in theta:
              theta[key] = theta[key] - eta*(grad["d" + key])

            # Initialise gradients
            grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

        train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
        epoch_data.append([i, train_error, train_loss, val_error, val_loss])
  
    return theta, epoch_data    
      

theta, epoch_data = stochastic_gradient_descent(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, 10, gamma)

# (d) RMSprop (Lecture 5)

eps = 10
beta = 0.95

# The following functions creates the square momenta for to be used in the 
# update rule in the RMSprop, adam optimiser

def init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size):
    size = [] # this contains the list of number of nodes of every layer in the network
    for j in range(number_hidden):
      size.append(hid_input_size)
    size = [net_input_size] + size + [net_output_size]
    momenta = {}
    for i in range(1, number_hidden+2):
        momenta["vW" + str(i)] = np.zeros((size[i], size[i-1]))
        momenta["vb" + str(i)] = np.zeros((size[i],1))
    return momenta


def RMSprop(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, batch_size, gamma):

  # Initialise paramaters

  theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )
  prev_momenta = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
  momenta = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
  points_covered = 0
  epoch_data = []

  # Loop
  for i in range(num_epochs):

      # Initialise gradients
      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

      for j in range(0,100):
        x = x_train[j,:]
        y = y_train[j,:]
        h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   
        grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)

        for key in grad:
            grad[key] = grad[key] + grad_t[key]
          
        points_covered = points_covered + 1

        if points_covered % batch_size == 0:

          # Update rule for RMSprop
          for keynew in theta:
            momenta["v" + keynew] = beta*prev_momenta["v"+ keynew] + (1 - beta)*((grad["d" + keynew])**2)
            theta[keynew] = theta[keynew] - (eta/ (np.sqrt(momenta["v" + keynew] + eps)) )*(grad["d" + keynew]) 
            prev_momenta["v" + keynew] = momenta["v" + keynew] 
          
          # Initialise gradients
          grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

      train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
      epoch_data.append([i, train_error, train_loss, val_error, val_loss])
  
  return theta, epoch_data    

    
theta, epoch_data = RMSprop(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, 10, gamma)

# (e) Adam optimiser (Lecture 5)

beta1 = 0.9
beta2 = 0.999
eps = 1e-4

def adam(num_epochs, act_func, loss_func, eta, number_hidden, hid_input_size, net_input_size, net_output_size, batch_size, beta1, beta2, eps):

    # Initialise paramaters

    theta = init_network( number_hidden, act_func, weight_init, hid_input_size, net_input_size, net_output_size )

    prev_momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta_bias = init_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)

    prev_momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta_sq = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)
    momenta_sq_bias = init_square_momenta(number_hidden, hid_input_size, net_input_size, net_output_size)

    points_covered = 0
    epoch_data = []

    # Loop
    for i in range(num_epochs):

      # Initialise gradients
      grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)
      time_count = 0

      for j in range(0,100):
          x = x_train[j,:]
          y = y_train[j,:]
          h, a, y_hat = forward_propogation(x, act_func, theta, number_hidden)   
          grad_t = backward_propogation(h, a, y, y_hat, theta, number_hidden, hid_input_size, net_input_size, net_output_size, loss_func, act_func)

          for key in grad:
              grad[key] = grad[key] + grad_t[key]
            
          points_covered = points_covered + 1

          if points_covered % batch_size == 0:
              time_count = time_count + 1

              for key in theta:

                  # First moments
                  momenta["m" + key] = beta1*prev_momenta["m" + key] + (1 - beta1) * grad["d" + key]
                  momenta_bias["m" + key] = momenta["m" + key]/(1 - np.power(beta1, time_count)) #bias correction

                  # Second moments
                  momenta_sq["v" + key] = beta2*prev_momenta_sq["v" + key] + (1 - beta2)*((grad["d" + key])**2)
                  momenta_sq_bias["v" + key] = momenta_sq["v" + key]/(1 - np.power(beta2, time_count)) #bias correction

                  #w_t+1
                  theta[key] = theta[key] - (eta/np.sqrt(momenta_sq_bias["v" + key] + eps))*momenta_bias["m" + key]

                  prev_momenta["m" + key] = momenta["m" + key]
                  prev_momenta_sq["v" + key] = momenta_sq["v" + key]
              
              # Initialise gradients
              grad = init_grad(number_hidden, hid_input_size, net_input_size, net_output_size)

      train_error, train_loss, val_error, val_loss = loss_accuracy_compute(x_train, y_train, x_val, y_val, theta, number_hidden, act_func, loss_func, net_output_size)
      epoch_data.append([i, train_error, train_loss, val_error, val_loss])
  
    return theta, epoch_data